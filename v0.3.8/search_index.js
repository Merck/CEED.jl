var documenterSearchIndex = {"docs":
[{"location":"tutorials/SimpleStatic.html","page":"Static Experimental Designs","title":"Static Experimental Designs","text":"EditURL = \"SimpleStatic.jl\"","category":"page"},{"location":"tutorials/SimpleStatic.html#simple_static","page":"Static Experimental Designs","title":"Static Experimental Designs","text":"","category":"section"},{"location":"tutorials/SimpleStatic.html","page":"Static Experimental Designs","title":"Static Experimental Designs","text":"In this document we describe the theoretical background behind the tools in CEEDesigns.jl for producing optimal \"static experimental designs,\" i.e., arrangements of experiments that exist along a Pareto-optimal tradeoff between cost and information gain. We also show an example with synthetic data.","category":"page"},{"location":"tutorials/SimpleStatic.html#Setting","page":"Static Experimental Designs","title":"Setting","text":"","category":"section"},{"location":"tutorials/SimpleStatic.html","page":"Static Experimental Designs","title":"Static Experimental Designs","text":"Consider the following scenario. There exists a set of experiments, each of which, when performed, yields measurements on one or more observables (features). Each subset of observables (and therefore each subset of experiments) has some \"information value,\" which is intentionally vaguely defined for generality, but for example, may be a loss function if that subset is used to train some machine learning model. It is generally the value of acquiring that information. Finally, each experiment has some monetary cost and execution time to perform the experiment, and the user has some known tradeoff between overall execution time and cost.","category":"page"},{"location":"tutorials/SimpleStatic.html","page":"Static Experimental Designs","title":"Static Experimental Designs","text":"CEEDesigns.jl provides tools to take these inputs and produce a set of optimal \"arrangements\" of experiments for each subset of experiments that form a Pareto front along the tradeoff between information gain and total combined cost (monetary and time). This allows informed decisions to be made, for example, regarding how to allocate scarce resources to a set of experiments that attain some acceptable level of information (or, conversely, reduce uncertainty below some level).","category":"page"},{"location":"tutorials/SimpleStatic.html","page":"Static Experimental Designs","title":"Static Experimental Designs","text":"The arrangements produced by the tools introduced in this tutorial are called \"static\" because they inherently assume that for each experiment, future data will deterministically yield the same information gain as the \"historical\" data did. This information gain from the \"historical\" data is quantified based on certain aggregate statistics.","category":"page"},{"location":"tutorials/SimpleStatic.html","page":"Static Experimental Designs","title":"Static Experimental Designs","text":"We can also consider \"generative experimental designs,\" where the information gain is modeled as a random variable. This concept is detailed in another tutorial.","category":"page"},{"location":"tutorials/SimpleStatic.html","page":"Static Experimental Designs","title":"Static Experimental Designs","text":"This tutorial introduces the theoretical framework behind static experimental designs with synthetic data. For examples using real data, please see our other tutorials.","category":"page"},{"location":"tutorials/SimpleStatic.html#Theoretical-Framework","page":"Static Experimental Designs","title":"Theoretical Framework","text":"","category":"section"},{"location":"tutorials/SimpleStatic.html#Experiments","page":"Static Experimental Designs","title":"Experiments","text":"","category":"section"},{"location":"tutorials/SimpleStatic.html","page":"Static Experimental Designs","title":"Static Experimental Designs","text":"Let E =  e_1 ldots e_n be a set of n experiments (i.e., E=n). Each experiment e in E has an associated tuple (m_et_e), giving the monetary cost and time duration required to perform experiment e.","category":"page"},{"location":"tutorials/SimpleStatic.html","page":"Static Experimental Designs","title":"Static Experimental Designs","text":"(Image: experiments)","category":"page"},{"location":"tutorials/SimpleStatic.html","page":"Static Experimental Designs","title":"Static Experimental Designs","text":"Consider P(E), the power set of experiments (i.e., every possible subset of experiments). Each subset of experiments Sin P(E) has an associated value v_S, which is the value of the experiments contained in S. This may be given by the loss function associated with a prediction task where the information yielded from S is used as predictor variables, or some other notion of information value.","category":"page"},{"location":"tutorials/SimpleStatic.html","page":"Static Experimental Designs","title":"Static Experimental Designs","text":"(Image: experiments)","category":"page"},{"location":"tutorials/SimpleStatic.html#Arrangements","page":"Static Experimental Designs","title":"Arrangements","text":"","category":"section"},{"location":"tutorials/SimpleStatic.html","page":"Static Experimental Designs","title":"Static Experimental Designs","text":"If experiments within a subset S can be performed simultaneously (in parallel), then each S may be arranged optimally with respect to time. An arrangement O_S of S is a partition of the experiments in S such that the size of each partition is not larger than the maximum number of experiments that may be done in parallel.","category":"page"},{"location":"tutorials/SimpleStatic.html","page":"Static Experimental Designs","title":"Static Experimental Designs","text":"Let l be the number of partitions, and o_i a partition in O_S. Then the arrangement is a surjection from S onto O_S. If no experiments can be done in parallel, then l=S. If all experiments are done in parallel, then l=1. Other arrangements fall between these extremes.","category":"page"},{"location":"tutorials/SimpleStatic.html","page":"Static Experimental Designs","title":"Static Experimental Designs","text":"(Image: experiments)","category":"page"},{"location":"tutorials/SimpleStatic.html#Optimal-Arrangements","page":"Static Experimental Designs","title":"Optimal Arrangements","text":"","category":"section"},{"location":"tutorials/SimpleStatic.html","page":"Static Experimental Designs","title":"Static Experimental Designs","text":"To find the optimal arrangement for each S we need to know the cost of O_S. The monetary cost of O_S is simply the sum of the costs of each experiment: m_O_S=sum_ein S m_e. The total time required is the sum of the maximum time of each partition. This is because while each partition in the arrangement is done in serial, experiments within partitions are done in parallel. That is, t_O_S=sum_i=1^l textmax  t_e e in o_i. Given these costs and a parameter lambda which controls the tradeoff between monetary cost and time, the combined cost of an arrangement is: lambda m_O_S + (1-lambda) t_O_S.","category":"page"},{"location":"tutorials/SimpleStatic.html","page":"Static Experimental Designs","title":"Static Experimental Designs","text":"For instance, consider the experiments S = e_1e_2e_3e_4, with associated costs (1 1), (1 3), (1 2), and (1 4). If we conduct experiments e_1 through e_4 in sequence, this would correspond to an arrangement O_S = ( e_1   e_2   e_3   e_4 ) with a total cost of m_O_S = 4 and t_O_S = 10.","category":"page"},{"location":"tutorials/SimpleStatic.html","page":"Static Experimental Designs","title":"Static Experimental Designs","text":"However, if we decide to conduct e_1 in parallel with e_3, and e_2 with e_4, we would obtain an arrangement O_S = ( e_1 e_3   e_2 e_4 ) with a total cost of m_O_S = 4, and t_O_S = 3 + 4 = 7.","category":"page"},{"location":"tutorials/SimpleStatic.html","page":"Static Experimental Designs","title":"Static Experimental Designs","text":"Continuing our example and assuming a maximum of two parallel experiments, the optimal arrangement is to conduct e_1 in parallel with e_2, and e_3 with e_4. This results in an arrangement O_S = ( e_1 e_2   e_3 e_4 ) with a total cost of m_o = 4 and t_o = 2 + 4 = 6.","category":"page"},{"location":"tutorials/SimpleStatic.html","page":"Static Experimental Designs","title":"Static Experimental Designs","text":"In fact, it can be readily demonstrated that the optimal arrangement can be found by ordering the experiments in S in descending order according to their execution times. Consequently, the experiments are grouped sequentially into partitions whose size equals the maximum number of parallel experiments, except possibly for the final set, if the maximum number of parallel experiments does not divide S evenly.","category":"page"},{"location":"tutorials/SimpleStatic.html#Synthetic-Data-Example","page":"Static Experimental Designs","title":"Synthetic Data Example","text":"","category":"section"},{"location":"tutorials/SimpleStatic.html","page":"Static Experimental Designs","title":"Static Experimental Designs","text":"We now present an example of finding cost-efficient designs for synthetic data using the CEEDesigns.jl package.","category":"page"},{"location":"tutorials/SimpleStatic.html","page":"Static Experimental Designs","title":"Static Experimental Designs","text":"First we load necessary packages.","category":"page"},{"location":"tutorials/SimpleStatic.html","page":"Static Experimental Designs","title":"Static Experimental Designs","text":"using CEEDesigns, CEEDesigns.StaticDesigns\nusing Combinatorics: powerset\nusing DataFrames\nusing POMDPs, POMDPTools, MCTS","category":"page"},{"location":"tutorials/SimpleStatic.html","page":"Static Experimental Designs","title":"Static Experimental Designs","text":"We consider a situation where there are 3 experiments, and we draw a value of their \"loss function\" or \"entropy\" from the uniform distribution on the unit interval for each.","category":"page"},{"location":"tutorials/SimpleStatic.html","page":"Static Experimental Designs","title":"Static Experimental Designs","text":"For each Sin P(E), we simulate the information value (v_S) of S as the product of the values for each individual experiment. Therefore, because smaller values are better, any subset containing multiple experiments is guaranteed to be more \"valuable\" than any component experiment.","category":"page"},{"location":"tutorials/SimpleStatic.html","page":"Static Experimental Designs","title":"Static Experimental Designs","text":"experiments = [\"e1\", \"e2\", \"e3\"];\nexperiments_val = Dict([e => rand() for e in experiments]);\n\nexperiments_evals = Dict(map(Set.(collect(powerset(experiments)))) do s\n    if length(s) > 0\n        s => prod([experiments_val[i] for i in s])\n    else\n        return s => 1.0\n    end\nend);\nnothing #hide","category":"page"},{"location":"tutorials/SimpleStatic.html","page":"Static Experimental Designs","title":"Static Experimental Designs","text":"See our other tutorial on heart disease triage for an example of using CEEDesigns.jl's built-in compatability with machine learning models from MLJ.jl to evalute performance of experiments using predictive accuracy as information value.","category":"page"},{"location":"tutorials/SimpleStatic.html","page":"Static Experimental Designs","title":"Static Experimental Designs","text":"Next we set up the costs (m_et_e) for each experiment. Better experiments are more costly, both in terms of time and monetary cost. We print the data frame showing each experiment and its associated costs.","category":"page"},{"location":"tutorials/SimpleStatic.html","page":"Static Experimental Designs","title":"Static Experimental Designs","text":"experiments_costs = Dict(\n    sort(collect(keys(experiments_val)); by = k -> experiments_val[k], rev = true) .=>\n        tuple.(1:3, 1:3),\n);\n\nDataFrame(;\n    experiment = collect(keys(experiments_costs)),\n    time = getindex.(values(experiments_costs), 1),\n    cost = getindex.(values(experiments_costs), 2),\n)","category":"page"},{"location":"tutorials/SimpleStatic.html","page":"Static Experimental Designs","title":"Static Experimental Designs","text":"We can plot the experiments ordered by their information value.","category":"page"},{"location":"tutorials/SimpleStatic.html","page":"Static Experimental Designs","title":"Static Experimental Designs","text":"plot_evals(\n    experiments_evals;\n    f = x -> sort(collect(keys(x)); by = k -> x[k], rev = true),\n    ylabel = \"loss\",\n)","category":"page"},{"location":"tutorials/SimpleStatic.html","page":"Static Experimental Designs","title":"Static Experimental Designs","text":"We print the data frame showing each subset of experiments and its overall information value.","category":"page"},{"location":"tutorials/SimpleStatic.html","page":"Static Experimental Designs","title":"Static Experimental Designs","text":"df_values = DataFrame(;\n    S = collect.(collect(keys(experiments_evals))),\n    value = collect(values(experiments_evals)),\n)\n\nsort(df_values, order(:value; rev = true))","category":"page"},{"location":"tutorials/SimpleStatic.html","page":"Static Experimental Designs","title":"Static Experimental Designs","text":"Now we are ready to find the subsets of experiments giving an optimal tradeoff between information value and combined cost. CEEDesigns exports a function efficient_designs which formulates the problem of finding optimal arrangements as a Markov Decision Process and solves optimal arrangements for each subset on the Pareto frontier.","category":"page"},{"location":"tutorials/SimpleStatic.html","page":"Static Experimental Designs","title":"Static Experimental Designs","text":"We set lambda=05, the parameter controlling the relative weight given to monetary versus time costs with the tuple tradeoff.","category":"page"},{"location":"tutorials/SimpleStatic.html","page":"Static Experimental Designs","title":"Static Experimental Designs","text":"max_parallel = 2;\ntradeoff = (0.5, 0.5);\n\ndesigns = efficient_designs(\n    experiments_costs,\n    experiments_evals;\n    max_parallel = max_parallel,\n    tradeoff = tradeoff,\n);\nnothing #hide","category":"page"},{"location":"tutorials/SimpleStatic.html","page":"Static Experimental Designs","title":"Static Experimental Designs","text":"Finally we may produce a plot of the set of cost-efficient experimental designs. The set of designs is plotted along a Pareto frontier giving tradeoff between informatio value (y-axis) and combined cost (x-axis). Note that because we set the maximum number of parallel experiments equal to 2, the efficient design for the complete set of experiments groups the experiments with long execution times together (see plot legend; each group within a partition is prefixed with a number).","category":"page"},{"location":"tutorials/SimpleStatic.html","page":"Static Experimental Designs","title":"Static Experimental Designs","text":"plot_front(designs; labels = make_labels(designs), ylabel = \"loss\")","category":"page"},{"location":"api.html#API-Documentation","page":"API Documentation","title":"API Documentation","text":"","category":"section"},{"location":"api.html","page":"API Documentation","title":"API Documentation","text":"CurrentModule = CEEDesigns","category":"page"},{"location":"api.html#StaticDesigns","page":"API Documentation","title":"StaticDesigns","text":"","category":"section"},{"location":"api.html","page":"API Documentation","title":"API Documentation","text":"CEEDesigns.StaticDesigns.efficient_designs\nCEEDesigns.StaticDesigns.evaluate_experiments","category":"page"},{"location":"api.html#CEEDesigns.StaticDesigns.efficient_designs","page":"API Documentation","title":"CEEDesigns.StaticDesigns.efficient_designs","text":"efficient_designs(experiments, evals; max_parallel=1, tradeoff=(1, 0), mdp_kwargs=default_mdp_kwargs)\n\nReturn the set of Pareto-efficient experimental designs, given experimental costs, predictive accuracy (loss), and estimated filtration rates for experimental subsets.\n\nArguments\n\nexperiments: a dictionary containing pairs experiment => cost (=> features), where cost can either be scalar cost or a tuple (monetary cost, execution time).\nevals: a dictionary containing pairs experimental subset => (; predictive loss, filtration).\n\nKeyword arguments\n\nparallel: to estimate the execution time of the design, define the number of experiments that can run concurrently. The experiments will subsequently be arranged in descending order based on their individual durations, and they will be then iteratively allocated into consecutive groups that represent parallel experiments.\ntradeoff: determines how to project the monetary cost and execution time of an experimental design onto a single combined cost.\n\nExample\n\nefficient_designs(\n    experiments_costs,\n    model,\n    data[!, Not(\"HeartDisease\")],\n    data[!, \"HeartDisease\"];\n    eval_options = (; zero_cost_features, measure = LogLoss()),\n    arrangement_options = (; max_parallel = 2, tradeoff = (0.0, 1)),\n)\n\n\n\n\n\nefficient_designs(experiments, args...; eval_options, arrangement_options)\n\nEvaluate predictive power for subsets of experiments, and return the set of Pareto-efficient experimental designs.\n\nInternally, evaluate_experiments is called first, followed by efficient_designs.\n\nKeyword arguments\n\neval_options: keyword arguments to evaluate_experiments.\narrangement_options: keyword arguments to efficient_designs.\n\nExample\n\nefficient_designs(\n    experiments_costs,\n    data_binary;\n    eval_options = (; zero_cost_features),\n    arrangement_options = (; max_parallel = 2, tradeoff = (0.0, 1)),\n)\n\n\n\n\n\n","category":"function"},{"location":"api.html#CEEDesigns.StaticDesigns.evaluate_experiments","page":"API Documentation","title":"CEEDesigns.StaticDesigns.evaluate_experiments","text":"evaluate_experiments(experiments, model, X, y; zero_cost_features=[], evaluate_empty_subset=true, return_full_metrics=false, kwargs...)\n\nEvaluate predictive accuracy over subsets of experiments, and return the metrics. The evaluation is facilitated by MLJ.evaluate; additional keyword arguments to this function will be passed to evaluate.\n\nEvaluations are run in parallel.\n\nArguments\n\nexperiments: a dictionary containing pairs experiment => (cost =>) features, where features is a subset of column names in data.\nmodel: a predictive model whose accuracy will be evaluated.\nX: a dataframe with features used for prediction.\ny: the variable that we aim to predict.\n\nKeyword arguments\n\nmax_cardinality: maximum cardinality of experimental subsets (defaults to the number of experiments).\nzero_cost_features: additional zero-cost features available for each experimental subset (defaults to an empty list).\nevaluate_empty_subset: flag indicating whether to evaluate empty experimental subset. A constant column will be added if zero_cost_features is empty (defaults to true).\nreturn_full_metrics: flag indicating whether to return full MLJ.PerformanceEvaluation metrics. Otherwise return an aggregate \"measurement\" for the first measure (defaults to false).\n\nExample\n\nevaluate_experiments(\n    experiments,\n    model,\n    data[!, Not(\"HeartDisease\")],\n    data[!, \"HeartDisease\"];\n    zero_cost_features,\n    measure = LogLoss(),\n)\n\n\n\n\n\nevaluate_experiments(experiments, X; zero_cost_features=[], evaluate_empty_subset=true)\n\nEvaluate discriminative power for subsets of experiments, and return the metrics.\n\nEvaluations are run in parallel.\n\nArguments\n\nexperiments: a dictionary containing pairs experiment => (cost =>) features, where features is a subset of column names in X.\nX: a dataframe containing binary labels, where false indicated that an entity was filtered out by the experiment (and should be removed from the triage).\n\nKeyword arguments\n\nzero_cost_features: additional zero-cost features available for each experimental subset (defaults to an empty list).\nevaluate_empty_subset: flag indicating whether to evaluate empty experimental subset.\n\nExample\n\nevaluate_experiments(experiments, data_binary; zero_cost_features)\n\n\n\n\n\n","category":"function"},{"location":"api.html#GenerativeDesigns","page":"API Documentation","title":"GenerativeDesigns","text":"","category":"section"},{"location":"api.html","page":"API Documentation","title":"API Documentation","text":"CEEDesigns.GenerativeDesigns.UncertaintyReductionMDP\nCEEDesigns.GenerativeDesigns.EfficientValueMDP\nCEEDesigns.GenerativeDesigns.State\nCEEDesigns.GenerativeDesigns.Variance\nCEEDesigns.GenerativeDesigns.Entropy","category":"page"},{"location":"api.html#CEEDesigns.GenerativeDesigns.UncertaintyReductionMDP","page":"API Documentation","title":"CEEDesigns.GenerativeDesigns.UncertaintyReductionMDP","text":"UncertaintyReductionMDP(costs; sampler, uncertainty, threshold, evidence=Evidence(), <keyword arguments>)\n\nStructure that parametrizes the experimental decision-making process. It is used in the object interface of POMDPs.\n\nIn this experimental setup, our objective is to minimize the expected experimental cost while ensuring the uncertainty remains below a specified threshold.\n\nInternally, a state of the decision process is modeled as a tuple (evidence::Evidence, [total accumulated monetary cost, total accumulated execution time]).\n\nArguments\n\ncosts: a dictionary containing pairs experiment => cost, where cost can either be a scalar cost (modelled as a monetary cost) or a tuple (monetary cost, execution time).\n\nKeyword Arguments\n\nsampler: a function of (evidence, features, rng), in which evidence denotes the current experimental evidence, features represent the set of features we want to sample from, and rng is a random number generator; it returns a dictionary mapping the features to outcomes.\nuncertainty: a function of evidence; it returns the measure of variance or uncertainty about the target variable, conditioned on the experimental evidence acquired so far.\nthreshold: a number representing the acceptable level of uncertainty about the target variable.\nevidence=Evidence(): initial experimental evidence.\ncosts_tradeoff: tradeoff between monetary cost and execution time of an experimental designs, given as a tuple of floats.\nmax_parallel: maximum number of parallel experiments.\ndiscount: this is the discounting factor utilized in reward computation.\nbigM: it refers to the penalty that arises in a scenario where further experimental action is not an option, yet the uncertainty exceeds the allowable limit.\nmax_experiments: this denotes the maximum number of experiments that are permissible to be conducted.\n\n\n\n\n\n","category":"type"},{"location":"api.html#CEEDesigns.GenerativeDesigns.EfficientValueMDP","page":"API Documentation","title":"CEEDesigns.GenerativeDesigns.EfficientValueMDP","text":"EfficientValueMDP(costs; sampler, value, evidence=Evidence(), <keyword arguments>)\n\nStructure that parametrizes the experimental decision-making process. It is used in the object interface of POMDPs.\n\nIn this experimental setup, our objective is to maximize the value of the experimental evidence (such as clinical utility), adjusted for experimental costs.\n\nInternally, the reward associated with a particular experimental evidence and with total accumulated monetary_cost and (optionally) execution_time is computed as value(evidence) - costs_tradeoff' * [monetary_cost, execution_time].\n\nArguments\n\ncosts: a dictionary containing pairs experiment => cost, where cost can either be a scalar cost (modelled as a monetary cost) or a tuple (monetary cost, execution time).\n\nKeyword Arguments\n\nsampler: a function of (evidence, features, rng), in which evidence denotes the current experimental evidence, features represent the set of features we want to sample from, and rng is a random number generator; it returns a dictionary mapping the features to outcomes.\nvalue: a function of (evidence); it quantifies the utility of experimental evidence.\nevidence=Evidence(): initial experimental evidence.\nmax_parallel: maximum number of parallel experiments.\ndiscount: this is the discounting factor utilized in reward computation.\n\n\n\n\n\n","category":"type"},{"location":"api.html#CEEDesigns.GenerativeDesigns.State","page":"API Documentation","title":"CEEDesigns.GenerativeDesigns.State","text":"Represent experimental state as a tuple of experimental costs and evidence.\n\n\n\n\n\n","category":"type"},{"location":"api.html#CEEDesigns.GenerativeDesigns.Variance","page":"API Documentation","title":"CEEDesigns.GenerativeDesigns.Variance","text":"Variance()\n\nReturn a function of (data; prior). When this function is called as part of an instantiation procedure in DistanceBased, it returns an internal function of weights that computes the fraction of variance in the data, relative to the variance calculated with respect to a specified prior.\n\n\n\n\n\n","category":"function"},{"location":"api.html#CEEDesigns.GenerativeDesigns.Entropy","page":"API Documentation","title":"CEEDesigns.GenerativeDesigns.Entropy","text":"Entropy()\n\nReturn a function of (labels; prior).  When this function is called as part of an instantiation procedure in DistanceBased, it returns an internal function of weights that computes the fraction of information entropy, relative to the entropy calculated with respect to a specified prior.\n\n\n\n\n\n","category":"function"},{"location":"api.html","page":"API Documentation","title":"API Documentation","text":"CEEDesigns.GenerativeDesigns.efficient_design\nCEEDesigns.GenerativeDesigns.efficient_designs\nCEEDesigns.GenerativeDesigns.efficient_value","category":"page"},{"location":"api.html#CEEDesigns.GenerativeDesigns.efficient_design","page":"API Documentation","title":"CEEDesigns.GenerativeDesigns.efficient_design","text":"efficient_design(costs; sampler, uncertainty, threshold, evidence=Evidence(), <keyword arguments>)\n\nIn the uncertainty reduction setup, minimize the expected experimental cost while ensuring the uncertainty remains below a specified threshold.\n\nArguments\n\ncosts: a dictionary containing pairs experiment => cost, where cost can either be a scalar cost (modelled as a monetary cost) or a tuple (monetary cost, execution time).\n\nKeyword Arguments\n\nsampler: a function of (evidence, features, rng), in which evidence denotes the current experimental evidence, features represent the set of features we want to sample from, and rng is a random number generator; it returns a dictionary mapping the features to outcomes.\nuncertainty: a function of evidence; it returns the measure of variance or uncertainty about the target variable, conditioned on the experimental evidence acquired so far.\nthreshold: uncertainty threshold.\nevidence=Evidence(): initial experimental evidence.\nsolver=default_solver: a POMDPs.jl compatible solver used to solve the decision process. The default solver is DPWSolver.\nrepetitions=0: number of runoffs used to estimate the expected experimental cost.\nmdp_options: a NamedTuple of additional keyword arguments that will be passed to the constructor of UncertaintyReductionMDP.\nrealized_uncertainty=false: whenever the initial state uncertainty is below the selected threshold, return the actual uncertainty of this state.\n\nExample\n\n(; sampler, uncertainty, weights) = DistanceBased(\n    data;\n    target = \"HeartDisease\",\n    uncertainty = Entropy(),\n    similarity = Exponential(; λ = 5),\n);\n# initialize evidence\nevidence = Evidence(\"Age\" => 35, \"Sex\" => \"M\")\n# set up solver (or use default)\nsolver = GenerativeDesigns.DPWSolver(; n_iterations = 60_000, tree_in_info = true)\ndesigns = efficient_design(\n    costs;\n    experiments,\n    sampler,\n    uncertainty,\n    threshold = 0.6,\n    evidence,\n    solver,            # planner\n    mdp_options = (; max_parallel = 1),\n    repetitions = 5,\n)\n\n\n\n\n\n","category":"function"},{"location":"api.html#CEEDesigns.GenerativeDesigns.efficient_designs","page":"API Documentation","title":"CEEDesigns.GenerativeDesigns.efficient_designs","text":"efficient_designs(costs; sampler, uncertainty, thresholds, evidence=Evidence(), <keyword arguments>)\n\nIn the uncertainty reduction setup, minimize the expected experimental resource spend over a range of uncertainty thresholds, and return the set of Pareto-efficient designs in the dimension of cost and uncertainty threshold.\n\nInternally, an instance of the UncertaintyReductionMDP structure is created for every selected uncertainty threshold and the corresponding runoffs are simulated.\n\nArguments\n\ncosts: a dictionary containing pairs experiment => cost, where cost can either be a scalar cost (modelled as a monetary cost) or a tuple (monetary cost, execution time).\n\nKeyword Arguments\n\nsampler: a function of (evidence, features, rng), in which evidence denotes the current experimental evidence, features represent the set of features we want to sample from, and rng is a random number generator; it returns a dictionary mapping the features to outcomes.\nuncertainty: a function of evidence; it returns the measure of variance or uncertainty about the target variable, conditioned on the experimental evidence acquired so far.\nthresholds: number of thresholds to consider uniformly in the range between 0 and 1, inclusive.\nevidence=Evidence(): initial experimental evidence.\nsolver=default_solver: a POMDPs.jl compatible solver used to solve the decision process. The default solver is DPWSolver.\nrepetitions=0: number of runoffs used to estimate the expected experimental cost.\nmdp_options: a NamedTuple of additional keyword arguments that will be passed to the constructor of UncertaintyReductionMDP.\nrealized_uncertainty=false: whenever the initial state uncertainty is below the selected threshold, return the actual uncertainty of this state.\n\nExample\n\n(; sampler, uncertainty, weights) = DistanceBased(\n    data;\n    target = \"HeartDisease\",\n    uncertainty = Entropy(),\n    similarity = Exponential(; λ = 5),\n);\n# initialize evidence\nevidence = Evidence(\"Age\" => 35, \"Sex\" => \"M\")\n# set up solver (or use default)\nsolver = GenerativeDesigns.DPWSolver(; n_iterations = 60_000, tree_in_info = true)\ndesigns = efficient_designs(\n    costs;\n    experiments,\n    sampler,\n    uncertainty,\n    thresholds = 6,\n    evidence,\n    solver,            # planner\n    mdp_options = (; max_parallel = 1),\n    repetitions = 5,\n)\n\n\n\n\n\n","category":"function"},{"location":"api.html#CEEDesigns.GenerativeDesigns.efficient_value","page":"API Documentation","title":"CEEDesigns.GenerativeDesigns.efficient_value","text":"efficient_value(costs; sampler, value, evidence=Evidence(), <keyword arguments>)\n\nEstimate the maximum value of experimental evidence (such as clinical utility), adjusted for experimental costs.\n\nInternally, an instance of the EfficientValueMDP structure is created and a summary over repetitions runoffs is returned.\n\nArguments\n\ncosts: a dictionary containing pairs experiment => cost, where cost can either be a scalar cost (modelled as a monetary cost) or a tuple (monetary cost, execution time).\n\nKeyword Arguments\n\nsampler: a function of (evidence, features, rng), in which evidence denotes the current experimental evidence, features represent the set of features we want to sample from, and rng is a random number generator; it returns a dictionary mapping the features to outcomes.\nvalue: a function of (evidence, (monetary costs, execution time)); it quantifies the utility of experimental evidence.\nevidence=Evidence(): initial experimental evidence.\nsolver=default_solver: a POMDPs.jl compatible solver used to solve the decision process. The default solver is DPWSolver.\nrepetitions=0: number of runoffs used to estimate the expected experimental cost.\nmdp_options: a NamedTuple of additional keyword arguments that will be passed to the constructor of EfficientValueMDP.\n\nExample\n\n(; sampler, uncertainty, weights) = DistanceBased(\n    data;\n    target = \"HeartDisease\",\n    uncertainty = Entropy(),\n    similarity = Exponential(; λ = 5),\n);\nvalue = (evidence, costs) -> (1 - uncertainty(evidence) + 0.005 * sum(costs));\n# initialize evidence\nevidence = Evidence(\"Age\" => 35, \"Sex\" => \"M\")\n# set up solver (or use default)\nsolver =\n    GenerativeDesigns.DPWSolver(; n_iterations = 10_000, depth = 3, tree_in_info = true)\ndesign = efficient_value(\n    experiments;\n    sampler,\n    value,\n    evidence,\n    solver,            # planner\n    mdp_options = (; max_parallel = 1),\n    repetitions = 5,\n)\n\n\n\n\n\n","category":"function"},{"location":"api.html#Distance-Based-Sampling","page":"API Documentation","title":"Distance-Based Sampling","text":"","category":"section"},{"location":"api.html","page":"API Documentation","title":"API Documentation","text":"CEEDesigns.GenerativeDesigns.DistanceBased\nCEEDesigns.GenerativeDesigns.QuadraticDistance\nCEEDesigns.GenerativeDesigns.DiscreteDistance\nCEEDesigns.GenerativeDesigns.SquaredMahalanobisDistance\nCEEDesigns.GenerativeDesigns.Exponential","category":"page"},{"location":"api.html#CEEDesigns.GenerativeDesigns.DistanceBased","page":"API Documentation","title":"CEEDesigns.GenerativeDesigns.DistanceBased","text":"DistanceBased(data; target, uncertainty=Entropy(), similarity=Exponential(), distance=Dict(); prior=ones(nrow(data)))\n\nCompute distances between experimental evidence and historical readouts, and apply a 'similarity' functional to obtain probability mass for each row.\n\nConsider using QuadraticDistance, DiscreteDistance, and SquaredMahalanobisDistance.\n\nReturn value\n\nA named tuple with the following fields:\n\nsampler: a function of (evidence, features, rng), in which evidence denotes the current experimental evidence, features represent the set of features we want to sample from, and rng is a random number generator; it returns a dictionary mapping the features to outcomes.\nuncertainty: a function of evidence; it returns the measure of variance or uncertainty about the target variable, conditioned on the experimental evidence acquired so far.\nweights: a function of evidence; it returns probabilities (posterior) acrss the rows in data.\n\nArguments\n\ndata: a dataframe with historical data.\ntarget: target column name or a vector of target columns names.\n\nKeyword Argumets\n\nuncertainty: a function that takes the subdataframe containing columns in targets along with prior, and returns an anonymous function taking a single argument (a probability vector over observations) and returns an uncertainty measure over targets.\nsimilarity: a function that, for each row, takes distances between row[col] and readout[col], and returns a non-negative probability mass for the row.\ndistance: a dictionary of pairs colname => similarity functional, where a similarity functional must implement the signature (readout, col; prior). Defaults to QuadraticDistance and DiscreteDistance for Continuous and Multiclass scitypes, respectively.\nprior: prior across rows, uniform by default.\nfilter_range: a dictionary of pairs colname => (lower bound, upper bound). If there's data in the current state for a specific column specified in this list, only historical observations within the defined range for that column are considered.\nimportance_weights: a dictionary of pairs colname with either weights or a function x -> weight, which will be applied to each element of the column to obtain the vector of weights. If data for a given column is available in the current state, the product of the corresponding weights is used to adjust the similarity vector.\n\nExample\n\n(; sampler, uncertainty, weights) = DistanceBased(\n    data;\n    target = \"HeartDisease\",\n    uncertainty = Entropy(),\n    similarity = Exponential(; λ = 5),\n);\n\n\n\n\n\n","category":"function"},{"location":"api.html#CEEDesigns.GenerativeDesigns.QuadraticDistance","page":"API Documentation","title":"CEEDesigns.GenerativeDesigns.QuadraticDistance","text":"QuadraticDistance(; λ=1, standardize=true)\n\nThis returns an anonymous function (x, col; prior) -> λ * (x .- col).^2 / σ. If standardize is set to true, σ represents col's variance calculated in relation to prior, otherwise σ equals one.\n\n\n\n\n\n","category":"function"},{"location":"api.html#CEEDesigns.GenerativeDesigns.DiscreteDistance","page":"API Documentation","title":"CEEDesigns.GenerativeDesigns.DiscreteDistance","text":"DiscreteDistance(; λ=1)\n\nReturn an anonymous function (x, col) -> λ * (x .== col).\n\n\n\n\n\n","category":"function"},{"location":"api.html#CEEDesigns.GenerativeDesigns.SquaredMahalanobisDistance","page":"API Documentation","title":"CEEDesigns.GenerativeDesigns.SquaredMahalanobisDistance","text":"SquaredMahalanobisDistance(; diagonal=0)\n\nReturns a function that computes squared Mahalanobis distance between each row of data and the evidence. For a singular covariance matrix, consider adding entries to the matrix's diagonal via the diagonal keyword.\n\nTo accommodate missing values, we have implemented an approach described in https://www.jstor.org/stable/3559861, on page 285.\n\nArguments\n\ndiagonal: A scalar to be added to the diagonal entries of the covariance matrix.\n\nReturns\n\nIt returns a high-level function of (data, targets, prior). When called, that function will return an internal function compute_distances that takes an Evidence and computes the squared Mahalanobis distance based on the input data and the evidence.\n\n\n\n\n\n","category":"function"},{"location":"api.html#CEEDesigns.GenerativeDesigns.Exponential","page":"API Documentation","title":"CEEDesigns.GenerativeDesigns.Exponential","text":"Exponential(; λ=1)\n\nReturn an anonymous function x -> exp(-λ * sum(x; init=0)).\n\n\n\n\n\n","category":"function"},{"location":"api.html#Plotting","page":"API Documentation","title":"Plotting","text":"","category":"section"},{"location":"api.html","page":"API Documentation","title":"API Documentation","text":"CEEDesigns.plot_front\nCEEDesigns.make_labels\nCEEDesigns.plot_evals","category":"page"},{"location":"api.html#CEEDesigns.plot_front","page":"API Documentation","title":"CEEDesigns.plot_front","text":"plot_front(designs; grad=cgrad(:Paired_12), xlabel, ylabel, labels=get_labels(designs))\n\nRender scatter plot of efficient designs, as returned from efficient_designs.\n\nYou may optionally specify a color gradient, to draw the colors from.\n\nExamples\n\ndesigns = efficient_designs(experiment, state)\nplot_front(designs)\nplot_front(designs; grad = cgrad(:Paired_12))\n\n\n\n\n\n","category":"function"},{"location":"api.html#CEEDesigns.make_labels","page":"API Documentation","title":"CEEDesigns.make_labels","text":"make_labels(designs)\n\nMake labels used plotting of experimental designs.\n\n\n\n\n\n","category":"function"},{"location":"api.html#CEEDesigns.plot_evals","page":"API Documentation","title":"CEEDesigns.plot_evals","text":"plot_evals(evals; f, ylabel=\"information measure\")\n\nCreate a stick plot that visualizes the performance measures evaluated for subsets of experiments.\n\nArgument evals should be the output of evaluate_experiments and the kwarg f (if provided) is a function that should take as input evals and return a list of its keys in the order to be plotted on the x-axis. By default they are sorted by length.\n\n\n\n\n\n","category":"function"},{"location":"tutorials/ActiveSampling.html","page":"Active Sampling for Generative Designs","title":"Active Sampling for Generative Designs","text":"EditURL = \"ActiveSampling.jl\"","category":"page"},{"location":"tutorials/ActiveSampling.html#generative_designs_active","page":"Active Sampling for Generative Designs","title":"Active Sampling for Generative Designs","text":"","category":"section"},{"location":"tutorials/ActiveSampling.html#Background-on-Active-Sampling","page":"Active Sampling for Generative Designs","title":"Background on Active Sampling","text":"","category":"section"},{"location":"tutorials/ActiveSampling.html","page":"Active Sampling for Generative Designs","title":"Active Sampling for Generative Designs","text":"In multi-objective optimization (MOO), particularly in the context of active learning and reinforcement learning (RL), conditional sampling plays a critical role in achieving optimized outcomes that align with specific desirable criteria. The essence of conditional sampling is to direct the generative process not only towards the global objectives but also to adhere to additional, domain-specific constraints or features. This approach is crucial for several reasons:","category":"page"},{"location":"tutorials/ActiveSampling.html","page":"Active Sampling for Generative Designs","title":"Active Sampling for Generative Designs","text":"MOO often involves balancing several competing objectives, such as minimizing cost while maximizing performance. Conditional sampling allows for the integration of additional constraints or preferences, ensuring that the optimization does not overly favor one objective at the expense of others.","category":"page"},{"location":"tutorials/ActiveSampling.html","page":"Active Sampling for Generative Designs","title":"Active Sampling for Generative Designs","text":"On a related note, many optimization problems have domain-specific constraints or desirable features that are not explicitly part of the primary objectives. For example, in drug design, beyond just optimizing for efficacy and safety, one might need to consider factors like solubility or synthesizability. Conditional sampling ensures that solutions not only meet the primary objectives but also align with these additional practical considerations.","category":"page"},{"location":"tutorials/ActiveSampling.html#Application-to-Generative-Designs","page":"Active Sampling for Generative Designs","title":"Application to Generative Designs","text":"","category":"section"},{"location":"tutorials/ActiveSampling.html","page":"Active Sampling for Generative Designs","title":"Active Sampling for Generative Designs","text":"In the context of CEEDesigns.jl, active sampling can be used to selectively prioritize historical observations. For example, if the goal is to understand a current trend or pattern, active sampling can be used to assign more weight to recent data points or deprioritize those that may not be relevant to the current context.","category":"page"},{"location":"tutorials/ActiveSampling.html","page":"Active Sampling for Generative Designs","title":"Active Sampling for Generative Designs","text":"This way, the sampled data will offer a more precise representation of the current state or trend.","category":"page"},{"location":"tutorials/ActiveSampling.html","page":"Active Sampling for Generative Designs","title":"Active Sampling for Generative Designs","text":"For details on the theoretical background of generative designs and notation, please see our introductory tutorial and an applied tutorial.","category":"page"},{"location":"tutorials/ActiveSampling.html","page":"Active Sampling for Generative Designs","title":"Active Sampling for Generative Designs","text":"Here we will again assume that the generative process is based on sampling from a historical dataset, which gives measurements on m features X = x_1 ldots x_m for l entities (with entities and features representing rows and columns, respectively). Each experiment e may yield measurements on some subset of features (X_esubseteq X).","category":"page"},{"location":"tutorials/ActiveSampling.html","page":"Active Sampling for Generative Designs","title":"Active Sampling for Generative Designs","text":"Given the current state, i.e., experimental evidence acquired thus far for the new entity, we assign probabilistic weights w_j over historical entities which are similar to the new entity. These weights can be used to weight the values of y or features associated with e_S^prime to construct approximations of q(ye_S) and q(e_S^primee_S).","category":"page"},{"location":"tutorials/ActiveSampling.html","page":"Active Sampling for Generative Designs","title":"Active Sampling for Generative Designs","text":"In the context of active sampling, we aim to further adjust the weights, w_j, calculated by the algorithm.","category":"page"},{"location":"tutorials/ActiveSampling.html","page":"Active Sampling for Generative Designs","title":"Active Sampling for Generative Designs","text":"This adjustment can be accomplished by introducing a \"prior\", which is essentially a vector of weights that will multiply the computed weights, w_j, in an element-wise manner. If we denote the \"prior\" weights as p_j, then the final weights assigned to the j-th row are computed as w_j = p_j * w_j.","category":"page"},{"location":"tutorials/ActiveSampling.html","page":"Active Sampling for Generative Designs","title":"Active Sampling for Generative Designs","text":"Alternatively, we can use \"feature-wise\" priors, which are considered when a readout for the specific feature is available for the new entity. # It is important to note here that the distance, which forms the basis of the probabilistic weights, is inherently computed only over the observed features.","category":"page"},{"location":"tutorials/ActiveSampling.html","page":"Active Sampling for Generative Designs","title":"Active Sampling for Generative Designs","text":"To be more precise, for each experiment ein E, we let p^e_j denote the \"prior\" associated with that specific experiment. If S represents the set of experiments that have been performed for the new compound so far, we compute the reweighted probabilistic weight as w_j = product_ein S p^e_j cdot w_j.","category":"page"},{"location":"tutorials/ActiveSampling.html","page":"Active Sampling for Generative Designs","title":"Active Sampling for Generative Designs","text":"We remark that this method can be used to filter out certain rows by setting their weight to zero.","category":"page"},{"location":"tutorials/ActiveSampling.html","page":"Active Sampling for Generative Designs","title":"Active Sampling for Generative Designs","text":"Considering feature-wise priors can offer a more detailed and nuanced understanding of the data. These priors can be used to dynamically adjust the weight of historical observations, based on the specific readouts considered for the observation across different features.","category":"page"},{"location":"tutorials/ActiveSampling.html","page":"Active Sampling for Generative Designs","title":"Active Sampling for Generative Designs","text":"For more information about active sampling, refer to the following articles.","category":"page"},{"location":"tutorials/ActiveSampling.html","page":"Active Sampling for Generative Designs","title":"Active Sampling for Generative Designs","text":"Evolutionary Multiobjective Optimization via Efficient Sampling-Based Strategies.\nSample-Efficient Multi-Objective Learning via Generalized Policy Improvement Prioritization.\nConditional gradient method for multiobjective optimization\nA practical guide to multi-objective reinforcement learning and planning","category":"page"},{"location":"tutorials/ActiveSampling.html#Synthetic-Data-Example","page":"Active Sampling for Generative Designs","title":"Synthetic Data Example","text":"","category":"section"},{"location":"tutorials/ActiveSampling.html","page":"Active Sampling for Generative Designs","title":"Active Sampling for Generative Designs","text":"using CEEDesigns, CEEDesigns.GenerativeDesigns","category":"page"},{"location":"tutorials/ActiveSampling.html","page":"Active Sampling for Generative Designs","title":"Active Sampling for Generative Designs","text":"We create a synthetic dataset with continuous variables, x1, x2, and y. Both x1 and x2 are modeled as independent random variables that follow a normal distribution. The target variable, y, is given as a weighted sum of x1 and x2, with an additional noise component. The corrected version of your sentence should be: Consequently, if the value of x2, for example, falls into a \"sparse\" region, we want the algorithm to avoid overfitting and focus its attention more on the other variable.","category":"page"},{"location":"tutorials/ActiveSampling.html","page":"Active Sampling for Generative Designs","title":"Active Sampling for Generative Designs","text":"using Random\nusing DataFrames\n\n# Define the size of the dataset.\nn = 1000;\n# Generate `x1` and `x2` as independent random variables with normal distribution.\nx1 = randn(n)\nx2 = randn(n);\n# Compute `y` as the sum of `x1`, `x2`, and the noise.\ny = x1 .+ 0.2 * x2 .+ 0.1 * randn(n);\n# Create a data frame.\ndata = DataFrame(; x1 = x1, x2 = x2, y = y);\ndata[1:10, :]","category":"page"},{"location":"tutorials/ActiveSampling.html#Active-Sampling","page":"Active Sampling for Generative Designs","title":"Active Sampling","text":"","category":"section"},{"location":"tutorials/ActiveSampling.html","page":"Active Sampling for Generative Designs","title":"Active Sampling for Generative Designs","text":"We again use the default method DistanceBased to assign the probabilistic weights across historical observations.","category":"page"},{"location":"tutorials/ActiveSampling.html","page":"Active Sampling for Generative Designs","title":"Active Sampling for Generative Designs","text":"In addition, we will demonstrate the use of two additional keyword arguments of DistanceBased, related to active sampling:","category":"page"},{"location":"tutorials/ActiveSampling.html","page":"Active Sampling for Generative Designs","title":"Active Sampling for Generative Designs","text":"importance_weights: this is a dictionary of pairs colname with either weights or a function x -> weight, which will be applied to each element of the column to obtain the vector of weights. If data for a given column is available in the current state, the product of the corresponding weights is used to adjust the similarity vector.","category":"page"},{"location":"tutorials/ActiveSampling.html","page":"Active Sampling for Generative Designs","title":"Active Sampling for Generative Designs","text":"For filtering out certain rows where the readouts fall outside a selected range, we can use the following keyword:","category":"page"},{"location":"tutorials/ActiveSampling.html","page":"Active Sampling for Generative Designs","title":"Active Sampling for Generative Designs","text":"filter_range: this is a dictionary of pairs colname => (lower bound, upper bound). If there's data in the current state for a specific column specified in this list, only historical observations within the defined range for that column are considered.","category":"page"},{"location":"tutorials/ActiveSampling.html","page":"Active Sampling for Generative Designs","title":"Active Sampling for Generative Designs","text":"In the current setup, we aim to adaptively filter out the values x1 or x2 that lie outside of one standard deviation from the mean.","category":"page"},{"location":"tutorials/ActiveSampling.html","page":"Active Sampling for Generative Designs","title":"Active Sampling for Generative Designs","text":"filter_range = Dict()\n\nfilter_range = Dict(\"x1\" => (-1, 1), \"x2\" => (-1, 1))","category":"page"},{"location":"tutorials/ActiveSampling.html","page":"Active Sampling for Generative Designs","title":"Active Sampling for Generative Designs","text":"We then call DistanceBased as follows:","category":"page"},{"location":"tutorials/ActiveSampling.html","page":"Active Sampling for Generative Designs","title":"Active Sampling for Generative Designs","text":"(sampler_active, uncertainty_active, weights_active) = DistanceBased(\n    data;\n    target = \"y\",\n    similarity = Exponential(; λ = 0.5),\n    filter_range = filter_range,\n);\nnothing #hide","category":"page"},{"location":"tutorials/ActiveSampling.html","page":"Active Sampling for Generative Designs","title":"Active Sampling for Generative Designs","text":"To compare behavior with and without active sampling, we call DistanceBased again:","category":"page"},{"location":"tutorials/ActiveSampling.html","page":"Active Sampling for Generative Designs","title":"Active Sampling for Generative Designs","text":"(; sampler, uncertainty, weights) =\n    DistanceBased(data; target = \"y\", similarity = Exponential(; λ = 0.5));\nnothing #hide","category":"page"},{"location":"tutorials/ActiveSampling.html","page":"Active Sampling for Generative Designs","title":"Active Sampling for Generative Designs","text":"We plot the weights w_j assigned to historical observations for both cases - with active sampling and without. The actual observation is shown in orange.","category":"page"},{"location":"tutorials/ActiveSampling.html","page":"Active Sampling for Generative Designs","title":"Active Sampling for Generative Designs","text":"evidence = Evidence(\"x1\" => 5.0, \"x2\" => 0.5)","category":"page"},{"location":"tutorials/ActiveSampling.html","page":"Active Sampling for Generative Designs","title":"Active Sampling for Generative Designs","text":"using Plots\n# The plotting engine (GR) requires us to use RGB instead of RGBA.\nrgba_to_rgb(a) = RGB(((1 - a) .* (1, 1, 1) .+ a .* (0.0, 0.5, 0.5))...)\nalphas_active = max.(0.1, weights_active(evidence) ./ maximum(weights_active(evidence)))\np1 = scatter(\n    data[!, \"x1\"],\n    data[!, \"x2\"];\n    color = map(a -> rgba_to_rgb(a), alphas_active),\n    title = \"weights\\n(active sampling)\",\n    mscolor = nothing,\n    colorbar = false,\n    label = false,\n)\nscatter!(\n    p1,\n    [evidence[\"x1\"]],\n    [evidence[\"x2\"]];\n    color = :orange,\n    mscolor = nothing,\n    label = nothing,\n)\np1","category":"page"},{"location":"tutorials/ActiveSampling.html","page":"Active Sampling for Generative Designs","title":"Active Sampling for Generative Designs","text":"alphas = max.(0.1, weights(evidence) ./ maximum(weights(evidence)))\np2 = scatter(\n    data[!, \"x1\"],\n    data[!, \"x2\"];\n    color = map(a -> rgba_to_rgb(a), alphas),\n    title = \"weights\\n(no active sampling)\",\n    mscolor = nothing,\n    colorbar = false,\n    label = false,\n)\nscatter!(\n    p2,\n    [evidence[\"x1\"]],\n    [evidence[\"x2\"]];\n    color = :orange,\n    mscolor = nothing,\n    label = nothing,\n)\np2","category":"page"},{"location":"tutorials/ActiveSampling.html","page":"Active Sampling for Generative Designs","title":"Active Sampling for Generative Designs","text":"As it turns out, when active sampling was not used, the algorithm tended to overfit to the closest yet sparse points, which did not represent the true distribution accurately. We can also compare the estimated uncertainty, which is computed as the variance of the posterior.","category":"page"},{"location":"tutorials/ActiveSampling.html","page":"Active Sampling for Generative Designs","title":"Active Sampling for Generative Designs","text":"Without using active sampling, we obtain:","category":"page"},{"location":"tutorials/ActiveSampling.html","page":"Active Sampling for Generative Designs","title":"Active Sampling for Generative Designs","text":"round(uncertainty_active(evidence); digits = 1)","category":"page"},{"location":"tutorials/ActiveSampling.html","page":"Active Sampling for Generative Designs","title":"Active Sampling for Generative Designs","text":"While for active sampling, we get:","category":"page"},{"location":"tutorials/ActiveSampling.html","page":"Active Sampling for Generative Designs","title":"Active Sampling for Generative Designs","text":"round(uncertainty(evidence); digits = 1)","category":"page"},{"location":"tutorials/ActiveSampling.html#Experimental-Designs-for-Uncertainty-Reduction","page":"Active Sampling for Generative Designs","title":"Experimental Designs for Uncertainty Reduction","text":"","category":"section"},{"location":"tutorials/ActiveSampling.html","page":"Active Sampling for Generative Designs","title":"Active Sampling for Generative Designs","text":"We compare the set of cost-efficient designs in cases where active sampling is used and where it is not.","category":"page"},{"location":"tutorials/ActiveSampling.html","page":"Active Sampling for Generative Designs","title":"Active Sampling for Generative Designs","text":"We specify the experiments along with the associated features:","category":"page"},{"location":"tutorials/ActiveSampling.html","page":"Active Sampling for Generative Designs","title":"Active Sampling for Generative Designs","text":"experiments = Dict(\"x1\" => 1.0, \"x2\" => 1.0, \"y\" => 6.0)","category":"page"},{"location":"tutorials/ActiveSampling.html","page":"Active Sampling for Generative Designs","title":"Active Sampling for Generative Designs","text":"We specify the initial state.","category":"page"},{"location":"tutorials/ActiveSampling.html","page":"Active Sampling for Generative Designs","title":"Active Sampling for Generative Designs","text":"evidence = Evidence(\"x2\" => 5.0)","category":"page"},{"location":"tutorials/ActiveSampling.html","page":"Active Sampling for Generative Designs","title":"Active Sampling for Generative Designs","text":"Next we compute the set of efficient designs.","category":"page"},{"location":"tutorials/ActiveSampling.html","page":"Active Sampling for Generative Designs","title":"Active Sampling for Generative Designs","text":"designs = efficient_designs(\n    experiments;\n    sampler,\n    uncertainty,\n    thresholds = 5,\n    evidence,\n    mdp_options = (; max_parallel = 1),\n);\n\ndesigns_active = efficient_designs(\n    experiments;\n    sampler = sampler_active,\n    uncertainty = uncertainty_active,\n    thresholds = 5,\n    evidence,\n    mdp_options = (; max_parallel = 1),\n);\nnothing #hide","category":"page"},{"location":"tutorials/ActiveSampling.html","page":"Active Sampling for Generative Designs","title":"Active Sampling for Generative Designs","text":"We can compare the fronts.","category":"page"},{"location":"tutorials/ActiveSampling.html","page":"Active Sampling for Generative Designs","title":"Active Sampling for Generative Designs","text":"p = scatter(\n    map(x -> x[1][1], designs),\n    map(x -> x[1][2], designs);\n    ylabel = \"% uncertainty\",\n    label = \"efficient designs (no active sampling)\",\n    title = \"efficient front\",\n    color = :blue,\n    mscolor = nothing,\n)\nscatter!(\n    p,\n    map(x -> x[1][1], designs_active),\n    map(x -> x[1][2], designs_active);\n    label = \"efficient designs (active sampling)\",\n    color = :teal,\n    mscolor = nothing,\n)\np","category":"page"},{"location":"tutorials/GenerativeDesigns.html","page":"Heart Disease Triage Meets Generative Modeling","title":"Heart Disease Triage Meets Generative Modeling","text":"EditURL = \"GenerativeDesigns.jl\"","category":"page"},{"location":"tutorials/GenerativeDesigns.html#generative_designs","page":"Heart Disease Triage Meets Generative Modeling","title":"Heart Disease Triage Meets Generative Modeling","text":"","category":"section"},{"location":"tutorials/GenerativeDesigns.html","page":"Heart Disease Triage Meets Generative Modeling","title":"Heart Disease Triage Meets Generative Modeling","text":"Consider again a situation where a group of patients is tested for a specific disease. It may be costly to conduct an experiment yielding the definitive answer; instead, we want to utilize various proxy experiments that provide partial information about the presence of the disease. For details on the theoretical background and notation, please see our tutorial on generative experimental designs. This tutorial is a concrete application of the tools described in that document.","category":"page"},{"location":"tutorials/GenerativeDesigns.html","page":"Heart Disease Triage Meets Generative Modeling","title":"Heart Disease Triage Meets Generative Modeling","text":"Importantly, we aim to design personalized adaptive policies for each patient. At the beginning of the triage process, we use a patient's prior data, such as sex, age, or type of chest pain, to project a range of cost-efficient experimental designs. Internally, while constructing these designs, we incorporate multiple-step-ahead lookups to model probable experimental outcomes and consider the subsequent decisions for each outcome. Then after choosing a specific decision policy from this set and acquiring additional experimental readouts, we adjust the continuation based on this evidence.","category":"page"},{"location":"tutorials/GenerativeDesigns.html#Heart-Disease-Dataset","page":"Heart Disease Triage Meets Generative Modeling","title":"Heart Disease Dataset","text":"","category":"section"},{"location":"tutorials/GenerativeDesigns.html","page":"Heart Disease Triage Meets Generative Modeling","title":"Heart Disease Triage Meets Generative Modeling","text":"In this tutorial, we consider a dataset that includes 11 clinical features along with a binary variable indicating the presence of heart disease in patients. The dataset can be found at Kaggle: Heart Failure Prediction. It utilizes heart disease datasets from the UCI Machine Learning Repository.","category":"page"},{"location":"tutorials/GenerativeDesigns.html","page":"Heart Disease Triage Meets Generative Modeling","title":"Heart Disease Triage Meets Generative Modeling","text":"using CSV, DataFrames\ndata = CSV.File(\"data/heart_disease.csv\") |> DataFrame\ndata[1:10, :]","category":"page"},{"location":"tutorials/GenerativeDesigns.html","page":"Heart Disease Triage Meets Generative Modeling","title":"Heart Disease Triage Meets Generative Modeling","text":"We provide appropriate scientific types of the features.","category":"page"},{"location":"tutorials/GenerativeDesigns.html","page":"Heart Disease Triage Meets Generative Modeling","title":"Heart Disease Triage Meets Generative Modeling","text":"using ScientificTypes\n\ntypes = Dict(\n    :MaxHR => Continuous,\n    :Cholesterol => Continuous,\n    :ChestPainType => Multiclass,\n    :Oldpeak => Continuous,\n    :HeartDisease => Multiclass,\n    :Age => Continuous,\n    :ST_Slope => Multiclass,\n    :RestingECG => Multiclass,\n    :RestingBP => Continuous,\n    :Sex => Multiclass,\n    :FastingBS => Continuous,\n    :ExerciseAngina => Multiclass,\n)\ndata = coerce(data, types);\nnothing #hide","category":"page"},{"location":"tutorials/GenerativeDesigns.html#Generative-Model-for-Outcomes-Sampling","page":"Heart Disease Triage Meets Generative Modeling","title":"Generative Model for Outcomes Sampling","text":"","category":"section"},{"location":"tutorials/GenerativeDesigns.html","page":"Heart Disease Triage Meets Generative Modeling","title":"Heart Disease Triage Meets Generative Modeling","text":"using CEEDesigns, CEEDesigns.GenerativeDesigns","category":"page"},{"location":"tutorials/GenerativeDesigns.html","page":"Heart Disease Triage Meets Generative Modeling","title":"Heart Disease Triage Meets Generative Modeling","text":"As previously discussed, we provide a dataset of historical records, the target variable, along with an information-theoretic measure to quantify the uncertainty about the target variable.","category":"page"},{"location":"tutorials/GenerativeDesigns.html","page":"Heart Disease Triage Meets Generative Modeling","title":"Heart Disease Triage Meets Generative Modeling","text":"In what follows, we obtain three functions:","category":"page"},{"location":"tutorials/GenerativeDesigns.html","page":"Heart Disease Triage Meets Generative Modeling","title":"Heart Disease Triage Meets Generative Modeling","text":"sampler: this is a function of (evidence, features, rng), in which evidence denotes the current experimental evidence, features represent the set of features we want to sample from, and rng is a random number generator,\nuncertainty: this is a function of evidence,\nweights: this represents a function of evidence that distributes probabilistic weights across the rows in the dataset.","category":"page"},{"location":"tutorials/GenerativeDesigns.html","page":"Heart Disease Triage Meets Generative Modeling","title":"Heart Disease Triage Meets Generative Modeling","text":"Note that internally, a state of the decision process is represented as a tuple (evidence, costs).","category":"page"},{"location":"tutorials/GenerativeDesigns.html","page":"Heart Disease Triage Meets Generative Modeling","title":"Heart Disease Triage Meets Generative Modeling","text":"You can specify the method for computing the distance using the distance keyword. By default, the Kronecker delta and quadratic distance will be utilised for categorical and continuous features, respectively.","category":"page"},{"location":"tutorials/GenerativeDesigns.html","page":"Heart Disease Triage Meets Generative Modeling","title":"Heart Disease Triage Meets Generative Modeling","text":"(; sampler, uncertainty, weights) = DistanceBased(\n    data;\n    target = \"HeartDisease\",\n    uncertainty = Entropy(),\n    similarity = Exponential(; λ = 5),\n);\nnothing #hide","category":"page"},{"location":"tutorials/GenerativeDesigns.html","page":"Heart Disease Triage Meets Generative Modeling","title":"Heart Disease Triage Meets Generative Modeling","text":"Alternatively, you can provide a dictionary of feature => distance pairs. The implemented distance functionals are DiscreteDistance(; λ) and QuadraticDistance(; λ, standardize=true). In that case, the specified distance will be applied to the respective feature, after which the distances will be collated across the range of features.","category":"page"},{"location":"tutorials/GenerativeDesigns.html","page":"Heart Disease Triage Meets Generative Modeling","title":"Heart Disease Triage Meets Generative Modeling","text":"The above call is therefore equivalent to:","category":"page"},{"location":"tutorials/GenerativeDesigns.html","page":"Heart Disease Triage Meets Generative Modeling","title":"Heart Disease Triage Meets Generative Modeling","text":"numeric_feats = filter(c -> c <: Real, eltype.(eachcol(data)))\ncategorical_feats = setdiff(names(data), numeric_feats)\n\nDistanceBased(\n    data;\n    target = \"HeartDisease\",\n    uncertainty = Entropy(),\n    similarity = Exponential(; λ = 5),\n    distance = merge(\n        Dict(c => DiscreteDistance() for c in categorical_feats),\n        Dict(c => QuadraticDistance() for c in numeric_feats),\n    ),\n);\nnothing #hide","category":"page"},{"location":"tutorials/GenerativeDesigns.html","page":"Heart Disease Triage Meets Generative Modeling","title":"Heart Disease Triage Meets Generative Modeling","text":"You can also use the squared Mahalanobis distance (SquaredMahalanobisDistance(; diagonal)). As the squared Mahalanobis distance only works with numeric features, we have to select a few, along with the target variable. For example, we could write:","category":"page"},{"location":"tutorials/GenerativeDesigns.html","page":"Heart Disease Triage Meets Generative Modeling","title":"Heart Disease Triage Meets Generative Modeling","text":"DistanceBased(\n    data[!, [\"RestingBP\", \"MaxHR\", \"Cholesterol\", \"FastingBS\", \"HeartDisease\"]];\n    target = \"HeartDisease\",\n    uncertainty = Entropy(),\n    similarity = Exponential(; λ = 5),\n    distance = SquaredMahalanobisDistance(; diagonal = 1),\n);\nnothing #hide","category":"page"},{"location":"tutorials/GenerativeDesigns.html","page":"Heart Disease Triage Meets Generative Modeling","title":"Heart Disease Triage Meets Generative Modeling","text":"The package offers an additional flexibility by allowing an experiment to yield readouts over multiple features at the same time. In our scenario, we can consider the features RestingECG, Oldpeak, ST_Slope, and MaxHR to be obtained from a single experiment ECG.","category":"page"},{"location":"tutorials/GenerativeDesigns.html","page":"Heart Disease Triage Meets Generative Modeling","title":"Heart Disease Triage Meets Generative Modeling","text":"We specify the experiments along with the associated features:","category":"page"},{"location":"tutorials/GenerativeDesigns.html","page":"Heart Disease Triage Meets Generative Modeling","title":"Heart Disease Triage Meets Generative Modeling","text":"experiments = Dict(\n    # experiment => features\n    \"BloodPressure\" => 1.0 => [\"RestingBP\"],\n    \"ECG\" => 5.0 => [\"RestingECG\", \"Oldpeak\", \"ST_Slope\", \"MaxHR\"],\n    \"BloodCholesterol\" => 20.0 => [\"Cholesterol\"],\n    \"BloodSugar\" => 20.0 => [\"FastingBS\"],\n    \"HeartDisease\" => 100.0,\n)","category":"page"},{"location":"tutorials/GenerativeDesigns.html","page":"Heart Disease Triage Meets Generative Modeling","title":"Heart Disease Triage Meets Generative Modeling","text":"Let us inspect the distribution of belief for the following experimental evidence:","category":"page"},{"location":"tutorials/GenerativeDesigns.html","page":"Heart Disease Triage Meets Generative Modeling","title":"Heart Disease Triage Meets Generative Modeling","text":"evidence = Evidence(\"Age\" => 55, \"Sex\" => \"M\")","category":"page"},{"location":"tutorials/GenerativeDesigns.html","page":"Heart Disease Triage Meets Generative Modeling","title":"Heart Disease Triage Meets Generative Modeling","text":"using StatsBase: countmap\nusing Plots","category":"page"},{"location":"tutorials/GenerativeDesigns.html","page":"Heart Disease Triage Meets Generative Modeling","title":"Heart Disease Triage Meets Generative Modeling","text":"target_belief = countmap(data[!, \"HeartDisease\"], weights(evidence))\np = bar(\n    0:1,\n    [target_belief[0], target_belief[1]];\n    xrot = 40,\n    ylabel = \"probability\",\n    color = :teal,\n    title = \"unc: $(round(uncertainty(evidence), digits=1))\",\n    kind = :bar,\n    legend = false,\n);\nxticks!(p, 0:1, [\"no disease\", \"disease\"]);\np","category":"page"},{"location":"tutorials/GenerativeDesigns.html","page":"Heart Disease Triage Meets Generative Modeling","title":"Heart Disease Triage Meets Generative Modeling","text":"Let us next add an outcome of blood pressure measurement:","category":"page"},{"location":"tutorials/GenerativeDesigns.html","page":"Heart Disease Triage Meets Generative Modeling","title":"Heart Disease Triage Meets Generative Modeling","text":"evidence_with_bp = merge(evidence, Dict(\"RestingBP\" => 190))\n\ntarget_belief = countmap(data[!, \"HeartDisease\"], weights(evidence_with_bp))\np = bar(\n    0:1,\n    [target_belief[0], target_belief[1]];\n    xrot = 40,\n    ylabel = \"probability\",\n    color = :teal,\n    title = \"unc: $(round(uncertainty(evidence_with_bp), digits=2))\",\n    kind = :bar,\n    legend = false,\n);\nxticks!(p, 0:1, [\"no disease\", \"disease\"]);\np","category":"page"},{"location":"tutorials/GenerativeDesigns.html#Cost-Efficient-Experimental-Designs-for-Uncertainty-Reduction","page":"Heart Disease Triage Meets Generative Modeling","title":"Cost-Efficient Experimental Designs for Uncertainty Reduction","text":"","category":"section"},{"location":"tutorials/GenerativeDesigns.html","page":"Heart Disease Triage Meets Generative Modeling","title":"Heart Disease Triage Meets Generative Modeling","text":"In this experimental setup, our objective is to minimize the expected experimental cost while ensuring the uncertainty remains below a specified threshold.","category":"page"},{"location":"tutorials/GenerativeDesigns.html","page":"Heart Disease Triage Meets Generative Modeling","title":"Heart Disease Triage Meets Generative Modeling","text":"We use the provided function efficient_designs to construct the set of cost-efficient experimental designs for various levels of uncertainty threshold. In the following example, we generate 6 thresholds spaces evenly between 0 and 1, inclusive.","category":"page"},{"location":"tutorials/GenerativeDesigns.html","page":"Heart Disease Triage Meets Generative Modeling","title":"Heart Disease Triage Meets Generative Modeling","text":"Internally, for each choice of the uncertainty threshold, an instance of a Markov decision problem in POMDPs.jl is created, and the POMDPs.solve is called on the problem. Afterwards, a number of simulations of the decision-making problem is run, all starting with the experimental state.","category":"page"},{"location":"tutorials/GenerativeDesigns.html","page":"Heart Disease Triage Meets Generative Modeling","title":"Heart Disease Triage Meets Generative Modeling","text":"# set seed for reproducibility\nusing Random: seed!\nseed!(1)","category":"page"},{"location":"tutorials/GenerativeDesigns.html","page":"Heart Disease Triage Meets Generative Modeling","title":"Heart Disease Triage Meets Generative Modeling","text":"evidence = Evidence(\"Age\" => 35, \"Sex\" => \"M\")","category":"page"},{"location":"tutorials/GenerativeDesigns.html","page":"Heart Disease Triage Meets Generative Modeling","title":"Heart Disease Triage Meets Generative Modeling","text":"# use less number of iterations to speed up build process\nsolver = GenerativeDesigns.DPWSolver(;\n    n_iterations = 20_000,\n    exploration_constant = 5.0,\n    tree_in_info = true,\n)\ndesigns = efficient_designs(\n    experiments;\n    sampler,\n    uncertainty,\n    thresholds = 6,\n    evidence,\n    solver,\n    mdp_options = (; max_parallel = 1),\n    repetitions = 5,\n);\nnothing #hide","category":"page"},{"location":"tutorials/GenerativeDesigns.html","page":"Heart Disease Triage Meets Generative Modeling","title":"Heart Disease Triage Meets Generative Modeling","text":"We plot the Pareto-efficient actions:","category":"page"},{"location":"tutorials/GenerativeDesigns.html","page":"Heart Disease Triage Meets Generative Modeling","title":"Heart Disease Triage Meets Generative Modeling","text":"plot_front(designs; labels = make_labels(designs), ylabel = \"% uncertainty\")","category":"page"},{"location":"tutorials/GenerativeDesigns.html","page":"Heart Disease Triage Meets Generative Modeling","title":"Heart Disease Triage Meets Generative Modeling","text":"We render the search tree for the second design, sorted in descending order based on the uncertainty threshold:","category":"page"},{"location":"tutorials/GenerativeDesigns.html","page":"Heart Disease Triage Meets Generative Modeling","title":"Heart Disease Triage Meets Generative Modeling","text":"using D3Trees\nd3tree = D3Tree(designs[2][2].tree; init_expand = 2)","category":"page"},{"location":"tutorials/GenerativeDesigns.html#Parallel-Experiments","page":"Heart Disease Triage Meets Generative Modeling","title":"Parallel Experiments","text":"","category":"section"},{"location":"tutorials/GenerativeDesigns.html","page":"Heart Disease Triage Meets Generative Modeling","title":"Heart Disease Triage Meets Generative Modeling","text":"We may exploit parallelism in the experimental arrangement. To that end, we first specify the monetary cost and execution time for each experiment, respectively.","category":"page"},{"location":"tutorials/GenerativeDesigns.html","page":"Heart Disease Triage Meets Generative Modeling","title":"Heart Disease Triage Meets Generative Modeling","text":"experiments = Dict(\n    # experiment => (monetary cost, execution time) => features\n    \"BloodPressure\" => (1.0, 1.0) => [\"RestingBP\"],\n    \"ECG\" => (5.0, 5.0) => [\"RestingECG\", \"Oldpeak\", \"ST_Slope\", \"MaxHR\"],\n    \"BloodCholesterol\" => (20.0, 20.0) => [\"Cholesterol\"],\n    \"BloodSugar\" => (20.0, 20.0) => [\"FastingBS\"],\n    \"HeartDisease\" => (100.0, 100.0),\n)","category":"page"},{"location":"tutorials/GenerativeDesigns.html","page":"Heart Disease Triage Meets Generative Modeling","title":"Heart Disease Triage Meets Generative Modeling","text":"We have to provide the maximum number of concurrent experiments. Additionally, we can specify the tradeoff between monetary cost and execution time. In our case, we aim to minimize the execution time.","category":"page"},{"location":"tutorials/GenerativeDesigns.html","page":"Heart Disease Triage Meets Generative Modeling","title":"Heart Disease Triage Meets Generative Modeling","text":"# minimize time, two concurrent experiments at maximum\nseed!(1)\n# use less number of iterations to speed up build process\nsolver = GenerativeDesigns.DPWSolver(;\n    n_iterations = 2_000,\n    exploration_constant = 5.0,\n    tree_in_info = true,\n)\ndesigns = efficient_designs(\n    experiments;\n    sampler,\n    uncertainty,\n    thresholds = 6,\n    evidence,\n    solver,\n    mdp_options = (; max_parallel = 2, costs_tradeoff = (0, 1.0)),\n    repetitions = 5,\n);\nnothing #hide","category":"page"},{"location":"tutorials/GenerativeDesigns.html","page":"Heart Disease Triage Meets Generative Modeling","title":"Heart Disease Triage Meets Generative Modeling","text":"We plot the Pareto-efficient actions:","category":"page"},{"location":"tutorials/GenerativeDesigns.html","page":"Heart Disease Triage Meets Generative Modeling","title":"Heart Disease Triage Meets Generative Modeling","text":"plot_front(designs; labels = make_labels(designs), ylabel = \"% uncertainty\")","category":"page"},{"location":"tutorials/GenerativeDesigns.html#Efficient-Value-Experimental-Designs","page":"Heart Disease Triage Meets Generative Modeling","title":"Efficient Value Experimental Designs","text":"","category":"section"},{"location":"tutorials/GenerativeDesigns.html","page":"Heart Disease Triage Meets Generative Modeling","title":"Heart Disease Triage Meets Generative Modeling","text":"In this experimental setup, we aim to maximize the value of experimental evidence, adjusted for the incurred experimental costs.","category":"page"},{"location":"tutorials/GenerativeDesigns.html","page":"Heart Disease Triage Meets Generative Modeling","title":"Heart Disease Triage Meets Generative Modeling","text":"For this purpose, we need to specify a function that quantifies the \"value\" of decision-process making state, modeled as a tuple of experimental evidence and costs.","category":"page"},{"location":"tutorials/GenerativeDesigns.html","page":"Heart Disease Triage Meets Generative Modeling","title":"Heart Disease Triage Meets Generative Modeling","text":"value = function (evidence, (monetary_cost, execution_time))\n    return (1 - uncertainty(evidence)) - (0.005 * sum(monetary_cost))\nend","category":"page"},{"location":"tutorials/GenerativeDesigns.html","page":"Heart Disease Triage Meets Generative Modeling","title":"Heart Disease Triage Meets Generative Modeling","text":"Considering a discount factor lambda, the total reward associated with the experimental state in an n-step decision process is given by r = r_1 + sum_i=2^n lambda^i-1 (r_i - r_i-1), where r_i is the value associated with the i-th state.","category":"page"},{"location":"tutorials/GenerativeDesigns.html","page":"Heart Disease Triage Meets Generative Modeling","title":"Heart Disease Triage Meets Generative Modeling","text":"In the following example, we also limit the maximum rollout horizon to 4.","category":"page"},{"location":"tutorials/GenerativeDesigns.html","page":"Heart Disease Triage Meets Generative Modeling","title":"Heart Disease Triage Meets Generative Modeling","text":"seed!(1)\n# use less number of iterations to speed up build process\nsolver = GenerativeDesigns.DPWSolver(; n_iterations = 2_000, depth = 4, tree_in_info = true)\ndesign = efficient_value(\n    experiments;\n    sampler,\n    value,\n    evidence,\n    solver,\n    repetitions = 5,\n    mdp_options = (; discount = 0.8),\n);\nnothing #hide","category":"page"},{"location":"tutorials/GenerativeDesigns.html","page":"Heart Disease Triage Meets Generative Modeling","title":"Heart Disease Triage Meets Generative Modeling","text":"design[1] # optimized cost-adjusted value","category":"page"},{"location":"tutorials/GenerativeDesigns.html","page":"Heart Disease Triage Meets Generative Modeling","title":"Heart Disease Triage Meets Generative Modeling","text":"d3tree = D3Tree(design[2].tree; init_expand = 2)","category":"page"},{"location":"tutorials/StaticDesignsFiltration.html","page":"Heart Disease Triage With Early Droupout","title":"Heart Disease Triage With Early Droupout","text":"EditURL = \"StaticDesignsFiltration.jl\"","category":"page"},{"location":"tutorials/StaticDesignsFiltration.html#static_designs_filtration","page":"Heart Disease Triage With Early Droupout","title":"Heart Disease Triage With Early Droupout","text":"","category":"section"},{"location":"tutorials/StaticDesignsFiltration.html","page":"Heart Disease Triage With Early Droupout","title":"Heart Disease Triage With Early Droupout","text":"Consider again a situation where a group of patients is tested for a specific disease. It may be costly to conduct an experiment yielding the definitive answer. Instead, we want to utilize various proxy experiments that provide partial information about the presence of the disease.","category":"page"},{"location":"tutorials/StaticDesignsFiltration.html","page":"Heart Disease Triage With Early Droupout","title":"Heart Disease Triage With Early Droupout","text":"Moreover, we may assume that for some patients, the evidence gathered from proxy experiments can be considered 'conclusive'. Effectively, some patients may not need any additional triage; they might be deemed healthy or require immediate commencement of treatment. By doing so, we could save significant resources.","category":"page"},{"location":"tutorials/StaticDesignsFiltration.html#Theoretical-Framework","page":"Heart Disease Triage With Early Droupout","title":"Theoretical Framework","text":"","category":"section"},{"location":"tutorials/StaticDesignsFiltration.html","page":"Heart Disease Triage With Early Droupout","title":"Heart Disease Triage With Early Droupout","text":"We take as a basis the setup and notation from the basic framework presented in the static experimental designs tutorial.","category":"page"},{"location":"tutorials/StaticDesignsFiltration.html","page":"Heart Disease Triage With Early Droupout","title":"Heart Disease Triage With Early Droupout","text":"We again have a set of experiments E, but now assume that a set of extrinsic decision-making rules is imposed on the experimental readouts. If the experimental evidence acquired for a given entity satisfies a specific criterion, that entity is then removed from the triage. However, other entities within the batch will generally continue in the experimental process. In general, the process of establishing such rules is largely dependent on the specific problem and requires comprehensive understanding of the subject area.","category":"page"},{"location":"tutorials/StaticDesignsFiltration.html","page":"Heart Disease Triage With Early Droupout","title":"Heart Disease Triage With Early Droupout","text":"We denote the expected fraction of entities that remain in the triage after conducting a set S of experiments as the filtration rate, f_S. In the context of disease triage, this can be interpreted as the fraction of patients for whom the experimental evidence does not provide a 'conclusive' result.","category":"page"},{"location":"tutorials/StaticDesignsFiltration.html","page":"Heart Disease Triage With Early Droupout","title":"Heart Disease Triage With Early Droupout","text":"As previously, each experiment e incurs a cost (m_e t_e). Again, we let O_S denote an arrangement of experiments in S.","category":"page"},{"location":"tutorials/StaticDesignsFiltration.html","page":"Heart Disease Triage With Early Droupout","title":"Heart Disease Triage With Early Droupout","text":"Given a subset S of experiments and their arrangement O_S, the total (discounted) monetary cost and execution time of the experimental design is given as m_o = sum_i=1^l r_S_i-1sum_ein o_i m_e and t_o = sum_i=1^l max  t_e  ein o_i, respectively. Importantly, the new factor r_S_i-1 models the fact that a set of entities may have dropped out in the previous experiments, hence saving the resources on running the subsequent experiments.","category":"page"},{"location":"tutorials/StaticDesignsFiltration.html","page":"Heart Disease Triage With Early Droupout","title":"Heart Disease Triage With Early Droupout","text":"We note that these computations are based on the assumption that monetary cost is associated with the analysis of a single experimental entity, such as a patient. Therefore, the total monetary cost obtained for a specific arrangement is effectively the \"expected\" monetary cost, adjusted for a single entity. Conversely, we suppose that all entities can be concurrently examined in a specific experiment. As such, the total execution time is equivalent to the longest time until all experiments within an arrangement are finished or all entities have been eliminated (which ocurrs when the filtration rate the experiments conducted so far is zero). Importantly, this distinctly differs from calculating the 'expected lifespan' of an entity in the triage.","category":"page"},{"location":"tutorials/StaticDesignsFiltration.html","page":"Heart Disease Triage With Early Droupout","title":"Heart Disease Triage With Early Droupout","text":"For instance, consider the experiments e_1 e_2 e_3, and e_4 with associated costs (1 1), (1 3), (1 2), and (1 4), and filtration rates 090807, and 06. For subsets of experiments, we simply assume that the filtration rates multiply, thereby treating the experiments as independent binary discriminators. In other words, the filtration rate for a set S= e_1 e_3  would equal f_S = 09 * 07 = 063.","category":"page"},{"location":"tutorials/StaticDesignsFiltration.html","page":"Heart Disease Triage With Early Droupout","title":"Heart Disease Triage With Early Droupout","text":"If we conduct experiments e_1 through e_4 in sequence, this would correspond to an arrangement o = ( e_1   e_2   e_3   e_4 ) with a total cost of m_o = 1 + 09 * 1 + 072 * 1 + 0504 * 1 = 3124 and t_o = 10.","category":"page"},{"location":"tutorials/StaticDesignsFiltration.html","page":"Heart Disease Triage With Early Droupout","title":"Heart Disease Triage With Early Droupout","text":"However, if we decide to conduct e_1 in parallel with e_3, and e_2 with e_4, we would obtain an arrangement o = ( e_1 e_3   e_2 e_4 ) with a total cost of m_o = 2 + 063 * 2 = 326, and t_o = 3 + 4 = 7.","category":"page"},{"location":"tutorials/StaticDesignsFiltration.html","page":"Heart Disease Triage With Early Droupout","title":"Heart Disease Triage With Early Droupout","text":"Given the constraint on the maximum number of parallel experiments, we construct an arrangement o of experiments S such that, for a fixed tradeoff lambda between monetary cost and execution time, the expected combined cost c_(o lambda) = lambda m_o + (1-lambda) t_o is minimized. Significantly, our objective is to leverage the filtration rates within the experimental arrangement.","category":"page"},{"location":"tutorials/StaticDesignsFiltration.html#A-Note-on-Optimal-Arrangements","page":"Heart Disease Triage With Early Droupout","title":"A Note on Optimal Arrangements","text":"","category":"section"},{"location":"tutorials/StaticDesignsFiltration.html","page":"Heart Disease Triage With Early Droupout","title":"Heart Disease Triage With Early Droupout","text":"In situations when experiments within a set S are executed sequentially, i.e., one after the other, it can be demonstrated that the experiments should be arranged in ascending order by fraclambda m_e + (1-lambda) t_e1-f_e.","category":"page"},{"location":"tutorials/StaticDesignsFiltration.html","page":"Heart Disease Triage With Early Droupout","title":"Heart Disease Triage With Early Droupout","text":"Continuing our example and assuming that the experiments are conducted sequentially, the optimal arrangement o would be to run experiments e_4 through e_1, yielding m_o = 2356.","category":"page"},{"location":"tutorials/StaticDesignsFiltration.html","page":"Heart Disease Triage With Early Droupout","title":"Heart Disease Triage With Early Droupout","text":"When we allow simultaneous execution of experiments, the problem turns more complicated, and we currently are not aware of an 'analytical' solution for it. Instead, we proposed approximating the 'optimal' arrangement as the 'optimal' policy found for a particular Markov decision process, in which:","category":"page"},{"location":"tutorials/StaticDesignsFiltration.html","page":"Heart Disease Triage With Early Droupout","title":"Heart Disease Triage With Early Droupout","text":"state is the set of experiments that have been conducted thus far,\nactions are subsets of experiments which have not been conducted yet; the size of these subsets is restricted by the maximum number of parallel experiments;\nreward is a combined monetary cost and execution time, discounted by the filtration rate of previously conducted experiments.","category":"page"},{"location":"tutorials/StaticDesignsFiltration.html","page":"Heart Disease Triage With Early Droupout","title":"Heart Disease Triage With Early Droupout","text":"Provided we know the information values v_S, filtration rates r_S, and experimental costs c_S for each set of experiments S, we find a collection of Pareto-efficient experimental designs that balance both the implied value of information and the experimental cost.","category":"page"},{"location":"tutorials/StaticDesignsFiltration.html#Heart-Disease-Dataset","page":"Heart Disease Triage With Early Droupout","title":"Heart Disease Dataset","text":"","category":"section"},{"location":"tutorials/StaticDesignsFiltration.html","page":"Heart Disease Triage With Early Droupout","title":"Heart Disease Triage With Early Droupout","text":"In this tutorial, we consider a dataset that includes 11 clinical features along with a binary variable indicating the presence of heart disease in patients. The dataset can be found at Kaggle: Heart Failure Prediction. It utilizes heart disease datasets from the UCI Machine Learning Repository.","category":"page"},{"location":"tutorials/StaticDesignsFiltration.html","page":"Heart Disease Triage With Early Droupout","title":"Heart Disease Triage With Early Droupout","text":"using CSV, DataFrames\ndata = CSV.File(\"data/heart_disease.csv\") |> DataFrame\ndata[1:10, :]","category":"page"},{"location":"tutorials/StaticDesignsFiltration.html","page":"Heart Disease Triage With Early Droupout","title":"Heart Disease Triage With Early Droupout","text":"In order to adapt the dataset to the current context, we can assume that, for every experiment, a medical specialist determined a range for 'conclusive' and 'inconclusive' outputs. This determination could be based on, say, optimizing the precision and recall factors of the resultant discriminative model. As an example, consider A novel approach for heart disease prediction using strength scores with significant predictors where rule mining for heart disease prediction is considered.\n It should be noted that the readout ranges defined below are entirely fictional.","category":"page"},{"location":"tutorials/StaticDesignsFiltration.html","page":"Heart Disease Triage With Early Droupout","title":"Heart Disease Triage With Early Droupout","text":"inconclusive_regions = Dict(\n    \"ChestPainType\" => [\"NAP\", \"ASY\"],\n    \"RestingBP\" => (50, 150),\n    \"ExerciseAngina\" => [\"N\"],\n    \"FastingBS\" => [0],\n    \"RestingECG\" => [\"Normal\"],\n    \"MaxHR\" => (50, 120),\n    \"Cholesterol\" => (0, 240),\n    \"Oldpeak\" => (-2.55, 2.55),\n)","category":"page"},{"location":"tutorials/StaticDesignsFiltration.html","page":"Heart Disease Triage With Early Droupout","title":"Heart Disease Triage With Early Droupout","text":"We apply the rules to derive a binary dataset where 'true' signifies that the readout was inconclusive, requiring them to remain in the triage.","category":"page"},{"location":"tutorials/StaticDesignsFiltration.html","page":"Heart Disease Triage With Early Droupout","title":"Heart Disease Triage With Early Droupout","text":"data_binary = DataFrame();\nfor colname in names(data[!, Not(\"HeartDisease\")])\n    if haskey(inconclusive_regions, colname)\n        if inconclusive_regions[colname] isa Vector\n            data_binary[!, colname] =\n                map(x -> x ∈ inconclusive_regions[colname], data[!, colname])\n        else\n            lb, ub = inconclusive_regions[colname]\n            data_binary[!, colname] = map(x -> lb <= x <= ub, data[!, colname])\n        end\n    else\n        data_binary[!, colname] = trues(nrow(data))\n    end\nend\n\ndata_binary[1:10, :]","category":"page"},{"location":"tutorials/StaticDesignsFiltration.html#Discriminative-Power-and-Filtration-Rates","page":"Heart Disease Triage With Early Droupout","title":"Discriminative Power and Filtration Rates","text":"","category":"section"},{"location":"tutorials/StaticDesignsFiltration.html","page":"Heart Disease Triage With Early Droupout","title":"Heart Disease Triage With Early Droupout","text":"In this scenario, we model the value of information v_S acquired by conducting a set of experiments as the ratio of patients for whom the results across the experiments in S were 'inconclusive', i.e., cap_ein S textpatient  textinconclusive in  e   textpatients. Essentially, the very same measure is used here to estimate the filtration rate.","category":"page"},{"location":"tutorials/StaticDesignsFiltration.html","page":"Heart Disease Triage With Early Droupout","title":"Heart Disease Triage With Early Droupout","text":"The CEEDesigns.jl package offers an additional flexibility by allowing an experiment to yield readouts over multiple features at the same time. In our scenario, we can consider the features RestingECG, Oldpeak, ST_Slope, and MaxHR to be obtained from a single experiment ECG.","category":"page"},{"location":"tutorials/StaticDesignsFiltration.html","page":"Heart Disease Triage With Early Droupout","title":"Heart Disease Triage With Early Droupout","text":"We specify the experiments along with the associated features:","category":"page"},{"location":"tutorials/StaticDesignsFiltration.html","page":"Heart Disease Triage With Early Droupout","title":"Heart Disease Triage With Early Droupout","text":"experiments = Dict(\n    # experiment => features\n    \"BloodPressure\" => [\"RestingBP\"],\n    \"ECG\" => [\"RestingECG\", \"Oldpeak\", \"ST_Slope\", \"MaxHR\"],\n    \"BloodCholesterol\" => [\"Cholesterol\"],\n    \"BloodSugar\" => [\"FastingBS\"],\n)","category":"page"},{"location":"tutorials/StaticDesignsFiltration.html","page":"Heart Disease Triage With Early Droupout","title":"Heart Disease Triage With Early Droupout","text":"We may also provide additional zero-cost features, which are always available.","category":"page"},{"location":"tutorials/StaticDesignsFiltration.html","page":"Heart Disease Triage With Early Droupout","title":"Heart Disease Triage With Early Droupout","text":"zero_cost_features = [\"Age\", \"Sex\", \"ChestPainType\", \"ExerciseAngina\"]","category":"page"},{"location":"tutorials/StaticDesignsFiltration.html","page":"Heart Disease Triage With Early Droupout","title":"Heart Disease Triage With Early Droupout","text":"For binary datasets, we may use evaluate_experiments from CEEDesigns.StaticDesigns to evaluate the discriminative power of subsets of experiments.","category":"page"},{"location":"tutorials/StaticDesignsFiltration.html","page":"Heart Disease Triage With Early Droupout","title":"Heart Disease Triage With Early Droupout","text":"using CEEDesigns, CEEDesigns.StaticDesigns","category":"page"},{"location":"tutorials/StaticDesignsFiltration.html","page":"Heart Disease Triage With Early Droupout","title":"Heart Disease Triage With Early Droupout","text":"perf_eval = evaluate_experiments(experiments, data_binary; zero_cost_features)","category":"page"},{"location":"tutorials/StaticDesignsFiltration.html","page":"Heart Disease Triage With Early Droupout","title":"Heart Disease Triage With Early Droupout","text":"Note that for each subset of experiments, the function returns a named tuple with fields loss and filtration.","category":"page"},{"location":"tutorials/StaticDesignsFiltration.html","page":"Heart Disease Triage With Early Droupout","title":"Heart Disease Triage With Early Droupout","text":"We plot discriminative power evaluated for subsets of experiments.","category":"page"},{"location":"tutorials/StaticDesignsFiltration.html","page":"Heart Disease Triage With Early Droupout","title":"Heart Disease Triage With Early Droupout","text":"plot_evals(perf_eval; ylabel = \"discriminative power\")","category":"page"},{"location":"tutorials/StaticDesignsFiltration.html#Cost-Efficient-Designs","page":"Heart Disease Triage With Early Droupout","title":"Cost-Efficient Designs","text":"","category":"section"},{"location":"tutorials/StaticDesignsFiltration.html","page":"Heart Disease Triage With Early Droupout","title":"Heart Disease Triage With Early Droupout","text":"We specify the cost associated with the execution of each experiment.","category":"page"},{"location":"tutorials/StaticDesignsFiltration.html","page":"Heart Disease Triage With Early Droupout","title":"Heart Disease Triage With Early Droupout","text":"costs = Dict(\n    # experiment => cost\n    \"BloodPressure\" => 1,\n    \"ECG\" => 5,\n    \"BloodCholesterol\" => 20,\n    \"BloodSugar\" => 20,\n)","category":"page"},{"location":"tutorials/StaticDesignsFiltration.html","page":"Heart Disease Triage With Early Droupout","title":"Heart Disease Triage With Early Droupout","text":"We use the provided function efficient_designs to construct the set of cost-efficient experimental designs. Note that the filtration is enabled implicitly since we provided the filtration rates within perf_eval. Another form of perf_eval would be subset of experiments => information measure, in which case the filtration would equal one. That is, no dropout would be considered.","category":"page"},{"location":"tutorials/StaticDesignsFiltration.html","page":"Heart Disease Triage With Early Droupout","title":"Heart Disease Triage With Early Droupout","text":"designs = efficient_designs(costs, perf_eval)","category":"page"},{"location":"tutorials/StaticDesignsFiltration.html","page":"Heart Disease Triage With Early Droupout","title":"Heart Disease Triage With Early Droupout","text":"plot_front(designs; labels = make_labels(designs), ylabel = \"discriminative power\")","category":"page"},{"location":"tutorials/StaticDesignsFiltration.html","page":"Heart Disease Triage With Early Droupout","title":"Heart Disease Triage With Early Droupout","text":"Let us compare the above with the efficient front with disabled filtration.","category":"page"},{"location":"tutorials/StaticDesignsFiltration.html","page":"Heart Disease Triage With Early Droupout","title":"Heart Disease Triage With Early Droupout","text":"# pass performance eval with discarded filtration rates (defaults to 1)\ndesigns_no_filtration = efficient_designs(costs, Dict(k => v.loss for (k, v) in perf_eval))","category":"page"},{"location":"tutorials/StaticDesignsFiltration.html","page":"Heart Disease Triage With Early Droupout","title":"Heart Disease Triage With Early Droupout","text":"using Plots\np = scatter(\n    map(x -> x[1][1], designs_no_filtration),\n    map(x -> x[1][2], designs_no_filtration);\n    label = \"designs with filtration disabled\",\n    c = :blue,\n    mscolor = nothing,\n    fontsize = 16,\n)\nscatter!(\n    p,\n    map(x -> x[1][1], designs),\n    map(x -> x[1][2], designs);\n    xlabel = \"combined cost\",\n    ylabel = \"discriminative power\",\n    label = \"designs with filtration enabled\",\n    c = :teal,\n    mscolor = nothing,\n    fontsize = 16,\n)","category":"page"},{"location":"tutorials/StaticDesignsFiltration.html#Arrangement-of-a-Set-of-Experiments","page":"Heart Disease Triage With Early Droupout","title":"Arrangement of a Set of Experiments","text":"","category":"section"},{"location":"tutorials/StaticDesignsFiltration.html","page":"Heart Disease Triage With Early Droupout","title":"Heart Disease Triage With Early Droupout","text":"Importantly, the total execution cost of an experiment is generally not the sum of costs associated to each individual experiment. In fact, a non-zero dropout rate (filtration < 1) discounts the expected cost of subsequent experiments. As discussed previously, we are not aware of an 'analytical' solution to this problem.","category":"page"},{"location":"tutorials/StaticDesignsFiltration.html","page":"Heart Disease Triage With Early Droupout","title":"Heart Disease Triage With Early Droupout","text":"Instead, we approximate the 'optimal' arrangement as the 'optimal' policy for a particular Markov decision process. This can be accomplished using, for instance, the Monte Carlo Tree Search algorithm as implemented in the POMDPs.jl package.","category":"page"},{"location":"tutorials/StaticDesignsFiltration.html","page":"Heart Disease Triage With Early Droupout","title":"Heart Disease Triage With Early Droupout","text":"The following is a visualisation of the DPW search tree that was used to find an optimal arrangement for a set of experiments yielding the highest information value.","category":"page"},{"location":"tutorials/StaticDesignsFiltration.html","page":"Heart Disease Triage With Early Droupout","title":"Heart Disease Triage With Early Droupout","text":"using MCTS, D3Trees\n\nexperiments = Set(vcat.(designs[end][2].arrangement...)[1])\n(; planner) = CEEDesigns.StaticDesigns.optimal_arrangement(\n    costs,\n    perf_eval,\n    experiments;\n    mdp_kwargs = (; tree_in_info = true),\n)\n_, info = action_info(planner, Set{String}())\n\nt = D3Tree(info[:tree]; init_expand = 2)","category":"page"},{"location":"tutorials/StaticDesignsFiltration.html","page":"Heart Disease Triage With Early Droupout","title":"Heart Disease Triage With Early Droupout","text":"plot_front(designs; labels = make_labels(designs), ylabel = \"discriminative power\")","category":"page"},{"location":"tutorials/StaticDesignsFiltration.html#Parallel-Experiments","page":"Heart Disease Triage With Early Droupout","title":"Parallel Experiments","text":"","category":"section"},{"location":"tutorials/StaticDesignsFiltration.html","page":"Heart Disease Triage With Early Droupout","title":"Heart Disease Triage With Early Droupout","text":"We may exploit parallelism in the experimental arrangement. To that end, we first specify the monetary cost and execution time for each experiment, respectively.","category":"page"},{"location":"tutorials/StaticDesignsFiltration.html","page":"Heart Disease Triage With Early Droupout","title":"Heart Disease Triage With Early Droupout","text":"experiments_costs = Dict(\n    # experiment => (monetary cost, execution time) => features\n    \"BloodPressure\" => (1.0, 1.0) => [\"RestingBP\"],\n    \"ECG\" => (5.0, 5.0) => [\"RestingECG\", \"Oldpeak\", \"ST_Slope\", \"MaxHR\"],\n    \"BloodCholesterol\" => (20.0, 20.0) => [\"Cholesterol\"],\n    \"BloodSugar\" => (20.0, 20.0) => [\"FastingBS\"],\n)","category":"page"},{"location":"tutorials/StaticDesignsFiltration.html","page":"Heart Disease Triage With Early Droupout","title":"Heart Disease Triage With Early Droupout","text":"We provide the maximum number of concurrent experiments. Additionally, we specify the tradeoff between monetary cost and execution time - in our case, we aim to minimize the execution time.","category":"page"},{"location":"tutorials/StaticDesignsFiltration.html","page":"Heart Disease Triage With Early Droupout","title":"Heart Disease Triage With Early Droupout","text":"Below, we demonstrate the flexibility of efficient_designs as it can both evaluate the performance of experiments and generate efficient designs. Internally, evaluate_experiments is called first, followed by efficient_designs. Keyword arguments to the respective functions has to wrapped in eval_options and arrangement_options named tuples, respectively.","category":"page"},{"location":"tutorials/StaticDesignsFiltration.html","page":"Heart Disease Triage With Early Droupout","title":"Heart Disease Triage With Early Droupout","text":"# Implicit, calculates accuracies automatically\ndesigns = efficient_designs(\n    experiments_costs,\n    data_binary;\n    eval_options = (; zero_cost_features),\n    arrangement_options = (; max_parallel = 2, tradeoff = (0.0, 1)),\n)","category":"page"},{"location":"tutorials/StaticDesignsFiltration.html","page":"Heart Disease Triage With Early Droupout","title":"Heart Disease Triage With Early Droupout","text":"As we can see, the algorithm correctly suggests running experiments in parallel.","category":"page"},{"location":"tutorials/StaticDesignsFiltration.html","page":"Heart Disease Triage With Early Droupout","title":"Heart Disease Triage With Early Droupout","text":"plot_front(designs; labels = make_labels(designs), ylabel = \"discriminative power\")","category":"page"},{"location":"tutorials/SimpleGenerative.html","page":"Generative Experimental Designs","title":"Generative Experimental Designs","text":"EditURL = \"SimpleGenerative.jl\"","category":"page"},{"location":"tutorials/SimpleGenerative.html#simple_generative","page":"Generative Experimental Designs","title":"Generative Experimental Designs","text":"","category":"section"},{"location":"tutorials/SimpleGenerative.html","page":"Generative Experimental Designs","title":"Generative Experimental Designs","text":"This document describes the theoretical background behind tools in CEEDesigns.jl for generative experimental designs and demonstrates uses on synthetic data examples.","category":"page"},{"location":"tutorials/SimpleGenerative.html#Setting","page":"Generative Experimental Designs","title":"Setting","text":"","category":"section"},{"location":"tutorials/SimpleGenerative.html","page":"Generative Experimental Designs","title":"Generative Experimental Designs","text":"Generative experimental designs differ from static designs in that the experimental design is specific for (or \"personalized\") to an incoming entity. Personalized cost-efficient experimental designs for the new entity are made based on existing information (if any) about the new entity, and from posterior distributions of unobserved features of that entity conditional on its observed features and historical data. The entities may be patients, molecular compounds, or any other objects where one has a store of historical data and seeks to find efficient experimental designs to learn more about a new arrival.","category":"page"},{"location":"tutorials/SimpleGenerative.html","page":"Generative Experimental Designs","title":"Generative Experimental Designs","text":"The personalized experimental designs are motivated by the fact that the value of information collected from an experiment often differs across subsets of the population of entities.","category":"page"},{"location":"tutorials/SimpleGenerative.html","page":"Generative Experimental Designs","title":"Generative Experimental Designs","text":"(Image: information value matrix)","category":"page"},{"location":"tutorials/SimpleGenerative.html","page":"Generative Experimental Designs","title":"Generative Experimental Designs","text":"In the context of static designs, we do not aspire to capture variation in information gain across different entities. Instead, we assume all entities come from a \"Population\" with a uniform information gain, in which case \"Experiment C\" would provide the maximum information value. On the other hand, if we have the ability to discern if the entity belong to subpopulations \"Population 1\" or \"Population 2,\" then we can tailor our design to suggest either \"Experiment A\" or \"Experiment B.\" Clearly, in the limit of a maximally heterogenous population, each entity has its own \"row.\" Our tools are able to handle the entire spectrum of such scenarios though distance based similarity, described below.","category":"page"},{"location":"tutorials/SimpleGenerative.html#Theoretical-Framework","page":"Generative Experimental Designs","title":"Theoretical Framework","text":"","category":"section"},{"location":"tutorials/SimpleGenerative.html#Historical-Data","page":"Generative Experimental Designs","title":"Historical Data","text":"","category":"section"},{"location":"tutorials/SimpleGenerative.html","page":"Generative Experimental Designs","title":"Generative Experimental Designs","text":"Like static designs, we consider a set E of n experiments, each with an associated tuple (m_et_e) of monetary and time costs (for more details on experiments and arrangements, see the tutorial on static experimental designs).","category":"page"},{"location":"tutorials/SimpleGenerative.html","page":"Generative Experimental Designs","title":"Generative Experimental Designs","text":"Additionally, consider a historical dataset giving measurements on m features X = x_1 ldots x_m for l entities (with entities and features representing rows and columns, respectively). Each experiment e may yield measurements on some subset of features (X_esubseteq X).","category":"page"},{"location":"tutorials/SimpleGenerative.html","page":"Generative Experimental Designs","title":"Generative Experimental Designs","text":"Furthermore there is an additional column y which is a target variable we want to predict (CEEDesigns.jl may allow y to be a vector, but we assume it is scalar here for ease of presentation).","category":"page"},{"location":"tutorials/SimpleGenerative.html","page":"Generative Experimental Designs","title":"Generative Experimental Designs","text":"Letting m=3, then the historical dataset may be visualized as the following table:","category":"page"},{"location":"tutorials/SimpleGenerative.html","page":"Generative Experimental Designs","title":"Generative Experimental Designs","text":"(Image: historical data)","category":"page"},{"location":"tutorials/SimpleGenerative.html#New-Entity","page":"Generative Experimental Designs","title":"New Entity","text":"","category":"section"},{"location":"tutorials/SimpleGenerative.html","page":"Generative Experimental Designs","title":"Generative Experimental Designs","text":"When a new entity arrives, it will have an unknown outcome y and unknown values of some or all features x_i in X. We call the state of the new entity the set of experiments conducted upon it so far (if any), along with the measurements on any features produced by those experiments (if any), called evidence.","category":"page"},{"location":"tutorials/SimpleGenerative.html","page":"Generative Experimental Designs","title":"Generative Experimental Designs","text":"Consider the case where there are n=3 experiments, each of which yields a measurement on a single feature. Then the state of a new entity arriving for which e_1 has already been performed will look like:","category":"page"},{"location":"tutorials/SimpleGenerative.html","page":"Generative Experimental Designs","title":"Generative Experimental Designs","text":"(Image: state)","category":"page"},{"location":"tutorials/SimpleGenerative.html","page":"Generative Experimental Designs","title":"Generative Experimental Designs","text":"Letting S denote the set of experiments which have been performed so far, then S^prime=ES are unperformed experiments. Then, actions one can perform to learn more about the new entity are subsets of S^prime. The size of subsets is limited by the maximum number of parallel experiments.","category":"page"},{"location":"tutorials/SimpleGenerative.html","page":"Generative Experimental Designs","title":"Generative Experimental Designs","text":"In our running example, if the maximum number of parallel experiments is at least 2, then the available actions are:","category":"page"},{"location":"tutorials/SimpleGenerative.html","page":"Generative Experimental Designs","title":"Generative Experimental Designs","text":"(Image: actions)","category":"page"},{"location":"tutorials/SimpleGenerative.html#Posterior-Distributions","page":"Generative Experimental Designs","title":"Posterior Distributions","text":"","category":"section"},{"location":"tutorials/SimpleGenerative.html","page":"Generative Experimental Designs","title":"Generative Experimental Designs","text":"Let e_S be a random variable denoting the outcome of some experiments S on the new entity. Then, there exists a posterior distribution q(e_S^primee_S) over outcomes of unperformed experiments S^prime, conditioned on the current state.","category":"page"},{"location":"tutorials/SimpleGenerative.html","page":"Generative Experimental Designs","title":"Generative Experimental Designs","text":"Furthermore, there also exists a posterior distribution over the unobserved target variable q(ye_S). The information value of the current state, derived from experimental evidence, can be defined as any statistical or information-theoretic measure applied to q(ye_S). This can include variance or entropy (among others). The information value of the set of experiments performed in S is analogous to v_S defined in static experimental designs.","category":"page"},{"location":"tutorials/SimpleGenerative.html#Similarity-Weighting","page":"Generative Experimental Designs","title":"Similarity Weighting","text":"","category":"section"},{"location":"tutorials/SimpleGenerative.html","page":"Generative Experimental Designs","title":"Generative Experimental Designs","text":"There may be many ways to define these posterior distributions, but our approach uses distance-based similarity scores to construct weights w_j over historical entities which are similar to the new entity. These weights can be used to weight the values of y or features associated with e_S^prime to construct approximations of q(ye_S) and q(e_S^primee_S).","category":"page"},{"location":"tutorials/SimpleGenerative.html","page":"Generative Experimental Designs","title":"Generative Experimental Designs","text":"If there is no evidence associated with a new entity, we assign w_j according to some prior distribution (uniform by default). Otherwise we use some particular distance and similarity metrics.","category":"page"},{"location":"tutorials/SimpleGenerative.html","page":"Generative Experimental Designs","title":"Generative Experimental Designs","text":"For each feature xin X, we consider a function rho_x, which measures the distance between two outputs. Please be aware that the \"distances\" discussed here do not conform to the mathematical definition of \"metrics\", even though they are functions derived from underlying metrics (i.e., a square of a metric). This is justified when considering how these \"distances\" are subsequently converted into probabilistic weights.","category":"page"},{"location":"tutorials/SimpleGenerative.html","page":"Generative Experimental Designs","title":"Generative Experimental Designs","text":"By default, we consider the following distances:","category":"page"},{"location":"tutorials/SimpleGenerative.html","page":"Generative Experimental Designs","title":"Generative Experimental Designs","text":"Rescaled Kronecker delta (i.e., rho(x y)=0 only when x=y, and rho(x y)= lambda under any other circumstances, with lambda  0) for discrete features (i.e., features whose types are modeled as MultiClass type in ScientificTypes.jl);\nRescaled (\"standardized\", by default) squared distance rho(x y) = λ frac(x - y)^2sigma^2, where sigma^2 is the variance of the feature column, estimated with respect to the prior for continuous features.\nSquared Mahalanobis distance rho(xy) = (x-y)^Sigma^-1(x-y), where Sigma is the empirical covariance matrix of the historical data.","category":"page"},{"location":"tutorials/SimpleGenerative.html","page":"Generative Experimental Designs","title":"Generative Experimental Designs","text":"For distance metrics assuming independence of features (Kronecker delta and squared distance), given the new entity's experimental state with readouts over the feature set F = bigcup X_e, where e in S, we can calculate the distance from the j-th historical entity as d_j = sum_xin F rho_x (hat x x_j), where hat x and x_j denote the readout for feature x for the entity being tested and the entity recorded in j-th column. The squared Mahalanobis distance directly takes in \"rows\", rho(hatxx_j).","category":"page"},{"location":"tutorials/SimpleGenerative.html","page":"Generative Experimental Designs","title":"Generative Experimental Designs","text":"Next, we convert distances d_j into probabilistic weights w_j. By default, we use a rescaled exponential function, i.e., we put w_j = exp(-lambda d_j) with lambda=05. Notably, lambda's value determines how belief is distributed across the historical entities. Larger values of lambda concentrate the belief tightly around the \"closest\" historical entities, while smaller values distribute more belief to more distant entities.","category":"page"},{"location":"tutorials/SimpleGenerative.html","page":"Generative Experimental Designs","title":"Generative Experimental Designs","text":"Note that by choosing the squared Mahalanobis distance and the exponential function with a factor of lambda=05, the resulting weight effectively equals the density of a multivariate normal distribution fitted to the historical data. A similar assertion applies when we use the sum of \"standardized\" squared distances instead. However, in this latter case, we \"enforce\" the independence of the features.","category":"page"},{"location":"tutorials/SimpleGenerative.html","page":"Generative Experimental Designs","title":"Generative Experimental Designs","text":"The proper choice of distance and similarity metrics depends on insight into the dataset at hand. Weights can then be used to construct weighted histograms or density estimators for the posterior distributions of interest, or to directly resample historical rows.","category":"page"},{"location":"tutorials/SimpleGenerative.html","page":"Generative Experimental Designs","title":"Generative Experimental Designs","text":"(Image: weights)","category":"page"},{"location":"tutorials/SimpleGenerative.html#Policy-Search","page":"Generative Experimental Designs","title":"Policy Search","text":"","category":"section"},{"location":"tutorials/SimpleGenerative.html","page":"Generative Experimental Designs","title":"Generative Experimental Designs","text":"Given these parts, searching for optimal experimental designs (actions arranged in an efficient way) depends on what our objective sense is.","category":"page"},{"location":"tutorials/SimpleGenerative.html","page":"Generative Experimental Designs","title":"Generative Experimental Designs","text":"The search may continue until the uncertainty about the posterior distribution of the target variable falls below a certain level. Our aim is to minimize the anticipated combined monetary cost and execution time of the search (considered as a \"negative\" reward). If all experiments are conducted without reaching below the required uncertainty level, or if the maximum number of experiments is exceeded, we penalize this scenario with an \"infinitely negative\" reward.\nWe may aim to minimize the expected uncertainty while being constrained by the combined costs of the experiment.\nAlternatively, we could maximize the value of experimental evidence, adjusted for the incurred experimental costs.","category":"page"},{"location":"tutorials/SimpleGenerative.html","page":"Generative Experimental Designs","title":"Generative Experimental Designs","text":"Standard Markov decision process (MDP) algorithms can be used to solve this problem (offline learning) or construct the policy (online learning) for the sequential decision-making.","category":"page"},{"location":"tutorials/SimpleGenerative.html","page":"Generative Experimental Designs","title":"Generative Experimental Designs","text":"Our MDP's state space is finite-dimensional but generally continuous due to the allowance of continuous features, which complicates the problem and few algorithms specialize in this area.","category":"page"},{"location":"tutorials/SimpleGenerative.html","page":"Generative Experimental Designs","title":"Generative Experimental Designs","text":"We used the Double Progressive Widening Algorithm for this task as detailed in A Comparison of Monte Carlo Tree Search and Mathematical Optimization for Large Scale Dynamic Resource Allocation.","category":"page"},{"location":"tutorials/SimpleGenerative.html","page":"Generative Experimental Designs","title":"Generative Experimental Designs","text":"In a nutshell, the Double Progressive Widening (DPW) algorithm is designed for online learning in complex environments, particularly those known as Continuous Finite-dimensional Markov Decision Processes where the state space is continuous. The key idea behind DPW is to progressively expand the search tree during the Monte Carlo Tree Search (MCTS) process. The algorithm does so by dynamically and selectively adding states and actions to the tree based on defined heuristics.","category":"page"},{"location":"tutorials/SimpleGenerative.html","page":"Generative Experimental Designs","title":"Generative Experimental Designs","text":"In the context of online learning, this algorithm addresses the complexity and challenges of real-time decision-making in domains with a large or infinite number of potential actions. As information is gathered in actual runtime, the algorithm explores and exploits this information to make optimal or near-optimal decisions. In other words, DPW permits the learning process to adapt on-the-fly as more data is made available, making it an effective tool for the dynamic and uncertain nature of online environments.","category":"page"},{"location":"tutorials/SimpleGenerative.html","page":"Generative Experimental Designs","title":"Generative Experimental Designs","text":"In particular, at the core of the Double Progressive Widening (DPW) algorithm are several key components, including expansion, search, and rollout.","category":"page"},{"location":"tutorials/SimpleGenerative.html","page":"Generative Experimental Designs","title":"Generative Experimental Designs","text":"The search component is where the algorithm sifts through the search tree to determine the most promising actions or states to explore next. By using exploration-exploitation strategies, it can effectively balance its efforts between investigating previously successful actions and venturing into unexplored territories.","category":"page"},{"location":"tutorials/SimpleGenerative.html","page":"Generative Experimental Designs","title":"Generative Experimental Designs","text":"The expansion phase is where the algorithm grows the search tree by adding new nodes, representing new state-action pairs, to the tree. This is done based on a predefined rule that dictates when and how much the tree should be expanded. This allows the algorithm to methodically discover and consider new potential actions without becoming overwhelmed with choices.","category":"page"},{"location":"tutorials/SimpleGenerative.html","page":"Generative Experimental Designs","title":"Generative Experimental Designs","text":"Lastly, the rollout stage, also known as a simulation stage, is where the algorithm plays out a series of actions to the end of a game or scenario using a specific policy (like a random policy). The results of these rollouts are then used to update the estimates of the values of state-action pairs, increasing the accuracy of future decisions.","category":"page"},{"location":"tutorials/SimpleGenerative.html","page":"Generative Experimental Designs","title":"Generative Experimental Designs","text":"(Image: One iteration of the MCTS algorithm, taken from https://ieeexplore.ieee.org/document/6145622)","category":"page"},{"location":"tutorials/SimpleGenerative.html","page":"Generative Experimental Designs","title":"Generative Experimental Designs","text":"In the above figure, nodes represent states of the decision process, while edges correspond to actions connecting these states.","category":"page"},{"location":"tutorials/SimpleGenerative.html","page":"Generative Experimental Designs","title":"Generative Experimental Designs","text":"A graphical summary of a single step of the overall search process to find the next best action, using our running example where a new entity has had e_1 performed out of 3 possible experiments is below:","category":"page"},{"location":"tutorials/SimpleGenerative.html","page":"Generative Experimental Designs","title":"Generative Experimental Designs","text":"(Image: search)","category":"page"},{"location":"tutorials/SimpleGenerative.html#Synthetic-Data-Example-with-Continuous-y","page":"Generative Experimental Designs","title":"Synthetic Data Example with Continuous y","text":"","category":"section"},{"location":"tutorials/SimpleGenerative.html","page":"Generative Experimental Designs","title":"Generative Experimental Designs","text":"We now present an example of finding cost-efficient generative designs on synthetic data using the CEEDesigns.jl package.","category":"page"},{"location":"tutorials/SimpleGenerative.html","page":"Generative Experimental Designs","title":"Generative Experimental Designs","text":"First we load necessary packages.","category":"page"},{"location":"tutorials/SimpleGenerative.html","page":"Generative Experimental Designs","title":"Generative Experimental Designs","text":"using CEEDesigns, CEEDesigns.GenerativeDesigns\nusing DataFrames\nusing ScientificTypes\nimport Distributions\nusing Copulas\nusing POMDPs, POMDPTools, MCTS\nusing Plots, StatsPlots, StatsBase","category":"page"},{"location":"tutorials/SimpleGenerative.html#Synthetic-Data","page":"Generative Experimental Designs","title":"Synthetic Data","text":"","category":"section"},{"location":"tutorials/SimpleGenerative.html","page":"Generative Experimental Designs","title":"Generative Experimental Designs","text":"We use the \"Friedman #3\" method to make synthetic data for a regression problem from scikit-learn. It considers m=4 features which predict a continuous y via some nonlinear transformations. The marginal distributions of each feature are given by the scikit-learn documentation. We additionally use a Gaussian copula to induce a specified correlation structure on the features to illustrate the difference between Euclidean and Mahalanobis distance metrics. We sample l=1000 rows of the historical data.","category":"page"},{"location":"tutorials/SimpleGenerative.html","page":"Generative Experimental Designs","title":"Generative Experimental Designs","text":"make_friedman3 = function (U, noise = 0, friedman3 = true)\n    size(U, 2) == 4 || error(\"input U must have 4 columns, has $(size(U,2))\")\n    n = size(U, 1)\n    X = DataFrame(zeros(Float64, n, 4), :auto)\n    for i = 1:4\n        X[:, i] .= U[:, i]\n    end\n    ϵ = noise > 0 ? rand(Distributions.Normal(0, noise), size(X, 1)) : 0\n    if friedman3\n        X.y = @. atan((X[:, 2] * X[:, 3] - 1 / (X[:, 2] * X[:, 4])) / X[:, 1]) + ϵ\n    else\n        # Friedman #2\n        X.y = @. (X[:, 1]^2 + (X[:, 2] * X[:, 3] - 1 / (X[:, 2] * X[:, 4]))^2)^0.5 + ϵ\n    end\n    return X\nend\n\nρ12, ρ13, ρ14, ρ23, ρ24, ρ34 = 0.8, 0.5, 0.3, 0.5, 0.25, 0.4\nΣ = [\n    1 ρ12 ρ13 ρ14\n    ρ12 1 ρ23 ρ24\n    ρ13 ρ23 1 ρ34\n    ρ14 ρ24 ρ34 1\n]\n\nX1 = Distributions.Uniform(0, 100)\nX2 = Distributions.Uniform(40 * π, 560 * π)\nX3 = Distributions.Uniform(0, 1)\nX4 = Distributions.Uniform(1, 11)\n\nC = GaussianCopula(Σ)\nD = SklarDist(C, (X1, X2, X3, X4))\n\nX = rand(D, 1000)\n\ndata = make_friedman3(transpose(X), 0.01)\n\ndata[1:10, :]","category":"page"},{"location":"tutorials/SimpleGenerative.html","page":"Generative Experimental Designs","title":"Generative Experimental Designs","text":"We can check that the empirical correlation is roughly the same as the specified theoretical values:","category":"page"},{"location":"tutorials/SimpleGenerative.html","page":"Generative Experimental Designs","title":"Generative Experimental Designs","text":"cor(Matrix(data[:, Not(:y)]))","category":"page"},{"location":"tutorials/SimpleGenerative.html","page":"Generative Experimental Designs","title":"Generative Experimental Designs","text":"We ensure that our algorithms know that we have provided data of specified types.","category":"page"},{"location":"tutorials/SimpleGenerative.html","page":"Generative Experimental Designs","title":"Generative Experimental Designs","text":"types = Dict(\n    :x1 => ScientificTypes.Continuous,\n    :x2 => ScientificTypes.Continuous,\n    :x3 => ScientificTypes.Continuous,\n    :x4 => ScientificTypes.Continuous,\n    :y => ScientificTypes.Continuous,\n)\ndata = coerce(data, types);\nnothing #hide","category":"page"},{"location":"tutorials/SimpleGenerative.html","page":"Generative Experimental Designs","title":"Generative Experimental Designs","text":"We may plot each feature x_i in X = x_1x_2x_3x_4 against y.","category":"page"},{"location":"tutorials/SimpleGenerative.html","page":"Generative Experimental Designs","title":"Generative Experimental Designs","text":"p1 = scatter(data.x1, data.y; legend = false, xlab = \"x1\")\np2 = scatter(data.x2, data.y; legend = false, xlab = \"x2\")\np3 = scatter(data.x3, data.y; legend = false, xlab = \"x3\")\np4 = scatter(data.x4, data.y; legend = false, xlab = \"x4\")\nplot(p1, p2, p3, p4; layout = (2, 2), legend = false)","category":"page"},{"location":"tutorials/SimpleGenerative.html#Distance-based-Similarity","page":"Generative Experimental Designs","title":"Distance-based Similarity","text":"","category":"section"},{"location":"tutorials/SimpleGenerative.html","page":"Generative Experimental Designs","title":"Generative Experimental Designs","text":"Given historical data, a target variable y, and metric to quantify uncertainty around the posterior distribution on the target q(ye_S), the function DistanceBased returns three functions needed by CEEDesigns.jl:","category":"page"},{"location":"tutorials/SimpleGenerative.html","page":"Generative Experimental Designs","title":"Generative Experimental Designs","text":"sampler: this is a function of (evidence, features, rng), in which evidence denotes the current experimental evidence, features represent the set of features we want to sample from, and rng is a random number generator;\nuncertainty: this is a function of evidence,\nweights: this represents a function of evidence that generates probability weights w_j to each row in the historical data.","category":"page"},{"location":"tutorials/SimpleGenerative.html","page":"Generative Experimental Designs","title":"Generative Experimental Designs","text":"By default, Euclidean distance is used as the distance metric. In the second call to DistanceBased, we instead use the squared Mahalanobis distance. It is possible to specify different distance metrics for each feature, see our heart disease generative modeling tutorial for more information. In both cases, the squared exponential function is used to convert distances to weights.","category":"page"},{"location":"tutorials/SimpleGenerative.html","page":"Generative Experimental Designs","title":"Generative Experimental Designs","text":"(; sampler, uncertainty, weights) = DistanceBased(\n    data;\n    target = \"y\",\n    uncertainty = Variance(),\n    similarity = GenerativeDesigns.Exponential(; λ = 5),\n);\n\n(sampler_mh, uncertainty_mh, weights_mh) = DistanceBased(\n    data;\n    target = \"y\",\n    uncertainty = Variance(),\n    similarity = GenerativeDesigns.Exponential(; λ = 5),\n    distance = SquaredMahalanobisDistance(; diagonal = 0),\n);\nnothing #hide","category":"page"},{"location":"tutorials/SimpleGenerative.html","page":"Generative Experimental Designs","title":"Generative Experimental Designs","text":"We can look at the uncertainty in y for a state where a single feature is \"observed\" at its mean value. As we consider only a single non-missing entry, note that the probabilistic weights assigned by the squared Mahalanobis distance are generally less \"spread out.\" This is because the variant of the squared Mahalanobis distance, which we implemented for handling missing values, effectively multiplies the other quadratic distance by a factor greater than one. For more details, refer to page 285 of Multivariate outlier detection in incomplete survey data: The epidemic algorithm and transformed rank correlations. The interaction with uncertainties is more complex as the uncertainty depends on the values of the target variable that correspond to the rows in the historical data.","category":"page"},{"location":"tutorials/SimpleGenerative.html","page":"Generative Experimental Designs","title":"Generative Experimental Designs","text":"data_uncertainties =\n    [i => uncertainty(Evidence(i => mean(data[:, i]))) for i in names(data)[1:4]]\nsort!(data_uncertainties; by = x -> x[2], rev = true)\n\ndata_uncertainties_mh =\n    [i => uncertainty_mh(Evidence(i => mean(data[:, i]))) for i in names(data)[1:4]]\nsort!(data_uncertainties_mh; by = x -> x[2], rev = true)\n\np1 = sticks(\n    eachindex(data_uncertainties),\n    [i[2] for i in data_uncertainties];\n    xformatter = i -> data_uncertainties[Int(i)][1],\n    label = false,\n    title = \"Uncertainty\\n(Euclidean distance)\",\n)\np2 = sticks(\n    eachindex(data_uncertainties_mh),\n    [i[2] for i in data_uncertainties_mh];\n    xformatter = i -> data_uncertainties_mh[Int(i)][1],\n    label = false,\n    title = \"Uncertainty\\n(Mahalanobis distance)\",\n)\n\nplot(p1, p2; layout = (1, 2), legend = false)","category":"page"},{"location":"tutorials/SimpleGenerative.html","page":"Generative Experimental Designs","title":"Generative Experimental Designs","text":"We can view the posterior distribution q(ye_S) conditioned on a state (here arbitrarily set to S = e_3, giving evidence for x_3).","category":"page"},{"location":"tutorials/SimpleGenerative.html","page":"Generative Experimental Designs","title":"Generative Experimental Designs","text":"evidence = Evidence(\"x3\" => mean(data.x3))\nplot_weights = StatsBase.weights(weights(evidence))\nplot_weights_mh = StatsBase.weights(weights_mh(evidence))\n\np1 = Plots.histogram(\n    data.y;\n    weights = plot_weights,\n    legend = false,\n    ylabel = \"Density\",\n    title = \"q(y|eₛ)\\n(Euclidean distance)\",\n)\np2 = Plots.histogram(\n    data.y;\n    weights = plot_weights_mh,\n    legend = false,\n    ylabel = \"Density\",\n    title = \"q(y|eₛ)\\n(Mahalanobis distance)\",\n)\n\nplot(p1, p2; layout = (1, 2), legend = false)","category":"page"},{"location":"tutorials/SimpleGenerative.html","page":"Generative Experimental Designs","title":"Generative Experimental Designs","text":"Like static designs, generative designs need to be provided a DataFrame assigning to each experiment a tuple of monetary and time costs (m_et_e), and what features each experiment provides observations of. We'll set up the experimental costs such that experiments which have less marginal uncertainty are more costly We finally add a very expensive \"final\" experiment which can directly observe the target variable.","category":"page"},{"location":"tutorials/SimpleGenerative.html","page":"Generative Experimental Designs","title":"Generative Experimental Designs","text":"observables_experiments = Dict([\"x$i\" => \"e$i\" for i = 1:4])\nexperiments_costs = Dict([\n    observables_experiments[e[1]] => (i, i) => [e[1]] for\n    (i, e) in enumerate(data_uncertainties_mh)\n])\n\nexperiments_costs[\"ey\"] = (100, 100) => [\"y\"]\n\nexperiments_costs_df =\n    DataFrame(; experiment = String[], time = Int[], cost = Int[], measurement = String[]);\npush!(\n    experiments_costs_df,\n    [\n        [\n            e,\n            experiments_costs[e][1][1],\n            experiments_costs[e][1][2],\n            only(experiments_costs[e][2]),\n        ] for e in keys(experiments_costs)\n    ]...,\n);\nexperiments_costs_df","category":"page"},{"location":"tutorials/SimpleGenerative.html#Find-Cost-efficient-Experimental-Designs","page":"Generative Experimental Designs","title":"Find Cost-efficient Experimental Designs","text":"","category":"section"},{"location":"tutorials/SimpleGenerative.html","page":"Generative Experimental Designs","title":"Generative Experimental Designs","text":"We can now find cost-efficient experimental designs for a new entity that has no measurements (Evidence()). Our objective sense is to minimize expected experimental combined cost while trying to reduce uncertainty to a threshold value. We examine 7 different threshold levels of uncertainty, evenly spaced between 0 and 1, inclusive. Additionally we set the costs_tradeoff such that equal weight is given to time and monetary cost when constructing the combined costs of experiments.","category":"page"},{"location":"tutorials/SimpleGenerative.html","page":"Generative Experimental Designs","title":"Generative Experimental Designs","text":"Internally, for each choice of the uncertainty threshold, an instance of a Markov decision problem in POMDPs.jl is created, and the POMDPs.solve is called on the problem. Afterwards, a number of simulations of the decision-making problem is run, all starting with the experimental state.","category":"page"},{"location":"tutorials/SimpleGenerative.html","page":"Generative Experimental Designs","title":"Generative Experimental Designs","text":"Note that we use the Euclidean distance, due to somewhat faster runtime.","category":"page"},{"location":"tutorials/SimpleGenerative.html","page":"Generative Experimental Designs","title":"Generative Experimental Designs","text":"n_thresholds = 7\nevidence = Evidence()\nsolver = GenerativeDesigns.DPWSolver(; n_iterations = 500, tree_in_info = true)\nrepetitions = 5\nmdp_options = (;\n    max_parallel = length(experiments_costs),\n    discount = 1.0,\n    costs_tradeoff = (0.5, 0.5),\n)\n\ndesigns = efficient_designs(\n    experiments_costs;\n    sampler = sampler,\n    uncertainty = uncertainty,\n    thresholds = n_thresholds,\n    evidence = evidence,\n    solver = solver,\n    repetitions = repetitions,\n    mdp_options = mdp_options,\n);\nnothing #hide","category":"page"},{"location":"tutorials/SimpleGenerative.html","page":"Generative Experimental Designs","title":"Generative Experimental Designs","text":"We plot the Pareto-efficient actions.","category":"page"},{"location":"tutorials/SimpleGenerative.html","page":"Generative Experimental Designs","title":"Generative Experimental Designs","text":"plot_front(designs; labels = make_labels(designs), ylabel = \"% uncertainty\")","category":"page"},{"location":"tutorials/SimpleGenerative.html#Synthetic-Data-Example-with-Discrete-y","page":"Generative Experimental Designs","title":"Synthetic Data Example with Discrete y","text":"","category":"section"},{"location":"tutorials/SimpleGenerative.html#Synthetic-Data-2","page":"Generative Experimental Designs","title":"Synthetic Data","text":"","category":"section"},{"location":"tutorials/SimpleGenerative.html","page":"Generative Experimental Designs","title":"Generative Experimental Designs","text":"We use n-class classification problem generator from scikit-learn We used parameters given below, for a total of m=5 features, with 4 of those informative and 1 redundant (linear combination of the other 4) feature. The y has 2 classes, with some added noise. We again sample l=1000 rows of historical entities.","category":"page"},{"location":"tutorials/SimpleGenerative.html","page":"Generative Experimental Designs","title":"Generative Experimental Designs","text":"The dataset can be approximately reproduced as below:","category":"page"},{"location":"tutorials/SimpleGenerative.html","page":"Generative Experimental Designs","title":"Generative Experimental Designs","text":"# using PyCall\n# sklearn = PyCall.pyimport_conda(\"sklearn\", \"scikit-learn\")\n# py\"\"\"\n# import sklearn as sk\n# from sklearn.datasets import make_classification\n# \"\"\"\n# X, y = py\"make_classification(n_samples=1000,n_features=5,n_informative=4,n_redundant=1,n_repeated=0,n_classes=2,n_clusters_per_class=2,flip_y=0.1)\"\n# using DataFrames, CSV\n# dat = DataFrame(X, :auto)\n# dat.y = y\n# CSV.write(\"./class.csv\",dat)\n\nusing CSV\n\ndata = CSV.read(\"./data/class.csv\", DataFrame)\n\ntypes = Dict(\n    :x1 => ScientificTypes.Continuous,\n    :x2 => ScientificTypes.Continuous,\n    :x3 => ScientificTypes.Continuous,\n    :x4 => ScientificTypes.Continuous,\n    :y => ScientificTypes.Multiclass,\n)\ndata = coerce(data, types);\nnothing #hide","category":"page"},{"location":"tutorials/SimpleGenerative.html#Distance-based-Similarity-2","page":"Generative Experimental Designs","title":"Distance-based Similarity","text":"","category":"section"},{"location":"tutorials/SimpleGenerative.html","page":"Generative Experimental Designs","title":"Generative Experimental Designs","text":"We now set up the distance based similarity functions. We use Entropy as our metric of uncertainty this time, which is more suitable for discrete y.","category":"page"},{"location":"tutorials/SimpleGenerative.html","page":"Generative Experimental Designs","title":"Generative Experimental Designs","text":"(; sampler, uncertainty, weights) = DistanceBased(\n    data;\n    target = \"y\",\n    uncertainty = Entropy(),\n    similarity = GenerativeDesigns.Exponential(; λ = 5),\n);\nnothing #hide","category":"page"},{"location":"tutorials/SimpleGenerative.html","page":"Generative Experimental Designs","title":"Generative Experimental Designs","text":"We may also look at the uncertainty from each marginal distribution of features. This is a bit nonsensical as the data generating function will create multimodal clusters so it will look artifically as if nothing is informative, but that is not the case.","category":"page"},{"location":"tutorials/SimpleGenerative.html","page":"Generative Experimental Designs","title":"Generative Experimental Designs","text":"data_uncertainties =\n    [i => uncertainty(Evidence(i => mode(data[:, i]))) for i in names(data)[1:end-1]]\nsort!(data_uncertainties; by = x -> x[2], rev = true)\n\nsticks(\n    eachindex(data_uncertainties),\n    [i[2] for i in data_uncertainties];\n    xformatter = i -> data_uncertainties[Int(i)][1],\n    label = false,\n    ylab = \"Uncertainty\",\n)","category":"page"},{"location":"tutorials/SimpleGenerative.html","page":"Generative Experimental Designs","title":"Generative Experimental Designs","text":"We can view the posterior distribution q(ye_S) when we consider the state as evidence a single measurement of the first feature, set to the mean of that distribution.","category":"page"},{"location":"tutorials/SimpleGenerative.html","page":"Generative Experimental Designs","title":"Generative Experimental Designs","text":"evidence = Evidence(\"x1\" => mean(data.x1))\nplot_weights = StatsBase.weights(weights(evidence))\n\ntarget_belief = countmap(data[!, \"y\"], plot_weights)\np = bar(\n    0:1,\n    [target_belief[0], target_belief[1]];\n    xrot = 40,\n    ylabel = \"probability\",\n    title = \"unc: $(round(uncertainty(evidence), digits=1))\",\n    kind = :bar,\n    legend = false,\n);\nxticks!(p, 0:1, [\"0\", \"1\"]);\np","category":"page"},{"location":"tutorials/SimpleGenerative.html","page":"Generative Experimental Designs","title":"Generative Experimental Designs","text":"Like previous examples, we'll set up the experimental costs such that experiments which have less marginal uncertainty are more costly, add a final very expensive experiment directly on the target variable.","category":"page"},{"location":"tutorials/SimpleGenerative.html","page":"Generative Experimental Designs","title":"Generative Experimental Designs","text":"observables_experiments = Dict([\"x$i\" => \"e$i\" for i = 1:5])\nexperiments_costs = Dict([\n    observables_experiments[e[1]] => (i, i) => [e[1]] for\n    (i, e) in enumerate(sort(data_uncertainties; by = x -> x[2], rev = true))\n])\n\nexperiments_costs[\"ey\"] = (100, 100) => [\"y\"]\n\nexperiments_costs_df =\n    DataFrame(; experiment = String[], time = Int[], cost = Int[], measurement = String[]);\npush!(\n    experiments_costs_df,\n    [\n        [\n            e,\n            experiments_costs[e][1][1],\n            experiments_costs[e][1][2],\n            only(experiments_costs[e][2]),\n        ] for e in keys(experiments_costs)\n    ]...,\n);\nexperiments_costs_df","category":"page"},{"location":"tutorials/SimpleGenerative.html#Find-Cost-efficient-Experimental-Designs-2","page":"Generative Experimental Designs","title":"Find Cost-efficient Experimental Designs","text":"","category":"section"},{"location":"tutorials/SimpleGenerative.html","page":"Generative Experimental Designs","title":"Generative Experimental Designs","text":"We can now find sets of cost-efficient experimental designs for a new entity with no measurements (Evidence()). We use the same solver parameters as for the exaple with continuous y, and plot the resulting Pareto front.","category":"page"},{"location":"tutorials/SimpleGenerative.html","page":"Generative Experimental Designs","title":"Generative Experimental Designs","text":"n_thresholds = 7\nevidence = Evidence()\nsolver = GenerativeDesigns.DPWSolver(; n_iterations = 500, tree_in_info = true)\nrepetitions = 5\nmdp_options = (;\n    max_parallel = length(experiments_costs),\n    discount = 1.0,\n    costs_tradeoff = (0.5, 0.5),\n)\n\ndesigns = efficient_designs(\n    experiments_costs;\n    sampler = sampler,\n    uncertainty = uncertainty,\n    thresholds = n_thresholds,\n    evidence = evidence,\n    solver = solver,\n    repetitions = repetitions,\n    mdp_options = mdp_options,\n);\n\nplot_front(designs; labels = make_labels(designs), ylabel = \"% uncertainty\")","category":"page"},{"location":"index.html#CEEDesigns.jl:-Overview","page":"CEEDesigns.jl: Overview","title":"CEEDesigns.jl: Overview","text":"","category":"section"},{"location":"index.html","page":"CEEDesigns.jl: Overview","title":"CEEDesigns.jl: Overview","text":"A decision-making framework for the cost-efficient design of experiments, balancing the value of acquired experimental evidence and incurred costs. We have considered two different experimental setups, which are outlined below.","category":"page"},{"location":"index.html","page":"CEEDesigns.jl: Overview","title":"CEEDesigns.jl: Overview","text":"<a><img src=\"assets/front_static.png\" align=\"right\" alt=\"code\" width=\"400\"></a>","category":"page"},{"location":"index.html#Static-experimental-designs","page":"CEEDesigns.jl: Overview","title":"Static experimental designs","text":"","category":"section"},{"location":"index.html","page":"CEEDesigns.jl: Overview","title":"CEEDesigns.jl: Overview","text":"Here we assume that the same experimental design will be used for a population of examined entities, hence the word \"static\".","category":"page"},{"location":"index.html","page":"CEEDesigns.jl: Overview","title":"CEEDesigns.jl: Overview","text":"For each subset of experiments, we consider an estimate of the value of acquired information. To give an example, if a set of experiments is used to predict the value of a specific target variable, our framework can leverage a built-in integration with MLJ.jl to estimate predictive accuracies of machine learning models fitted over subset of experimental features.","category":"page"},{"location":"index.html","page":"CEEDesigns.jl: Overview","title":"CEEDesigns.jl: Overview","text":"In the cost-sensitive setting of CEEDesigns.jl, a user provides the monetary cost and execution time of each experiment. Given the constraint on the maximum number of parallel experiments along with a fixed tradeoff between monetary cost and execution time, we devise an arrangement of each subset of experiments such that the expected combined cost is minimized.","category":"page"},{"location":"index.html","page":"CEEDesigns.jl: Overview","title":"CEEDesigns.jl: Overview","text":"Assuming the information values and optimized experimental costs for each subset of experiments, we then generate a set of cost-efficient experimental designs.","category":"page"},{"location":"index.html","page":"CEEDesigns.jl: Overview","title":"CEEDesigns.jl: Overview","text":"<a><img src=\"assets/front_generative.png\" align=\"right\" alt=\"code\" width=\"400\"></a>","category":"page"},{"location":"index.html#Generative-experimental-designs","page":"CEEDesigns.jl: Overview","title":"Generative experimental designs","text":"","category":"section"},{"location":"index.html","page":"CEEDesigns.jl: Overview","title":"CEEDesigns.jl: Overview","text":"We consider 'personalized' experimental designs that dynamically adjust based on the evidence gathered from the experiments. This approach is motivated by the fact that the value of information collected from an experiment generally differs across subpopulations of the entities involved in the triage process.","category":"page"},{"location":"index.html","page":"CEEDesigns.jl: Overview","title":"CEEDesigns.jl: Overview","text":"At the beginning of the triage process, an entity's prior data is used to project a range of cost-efficient experimental designs. Internally, while constructing these designs, we incorporate multiple-step-ahead lookups to model likely experimental outcomes and consider the subsequent decisions for each outcome. Then after choosing a specific decision policy from this set and acquiring additional experimental readouts (sampled from a generative model, hence the word \"generative\"), we adjust the continuation based on this evidence.","category":"page"},{"location":"index.html","page":"CEEDesigns.jl: Overview","title":"CEEDesigns.jl: Overview","text":"<a><img src=\"assets/search_tree.png\" align=\"left\" alt=\"code\" width=\"400\"></a>","category":"page"},{"location":"index.html","page":"CEEDesigns.jl: Overview","title":"CEEDesigns.jl: Overview","text":"We conceptualized the triage as a Markov decision process, in which we iteratively choose to conduct a subset of experiments and then, based on the experimental evidence, update our belief about the distribution of outcomes for the experiments that have not yet been conducted. The information value associated with the state, derived from experimental evidence, can be modeled through any statistical or information-theoretic measure such as the variance or uncertainty associated with the target variable posterior.","category":"page"},{"location":"index.html","page":"CEEDesigns.jl: Overview","title":"CEEDesigns.jl: Overview","text":"We implemented the following two variants of the decision-making process: Firstly, assuming that the decision-making process only terminates when the uncertainty drops below a given threshold, we minimize the expected resource spend. Secondly, we can optimize the value of experimental evidence, adjusted for the incurred experimental costs.","category":"page"},{"location":"tutorials/StaticDesigns.html","page":"Heart Disease Triage","title":"Heart Disease Triage","text":"EditURL = \"StaticDesigns.jl\"","category":"page"},{"location":"tutorials/StaticDesigns.html#static_designs","page":"Heart Disease Triage","title":"Heart Disease Triage","text":"","category":"section"},{"location":"tutorials/StaticDesigns.html","page":"Heart Disease Triage","title":"Heart Disease Triage","text":"Consider a situation where a group of patients is tested for a specific disease. It may be costly to conduct an experiment yielding the definitive answer. Instead, we want to utilize various proxy experiments that provide partial information about the presence of the disease. For details on the theoretical background and notation, please see our tutorial on static experimental designs, this tutorial is a concrete application of the tools described in that document.","category":"page"},{"location":"tutorials/StaticDesigns.html#Application-to-Predictive-Modeling","page":"Heart Disease Triage","title":"Application to Predictive Modeling","text":"","category":"section"},{"location":"tutorials/StaticDesigns.html","page":"Heart Disease Triage","title":"Heart Disease Triage","text":"Let's generalize the example from static experimental designs to the case where we want to compute the information value v_S as the predictive ability of a machine learning model which uses the measurements gained from experiments in S to predict some y of interest.","category":"page"},{"location":"tutorials/StaticDesigns.html","page":"Heart Disease Triage","title":"Heart Disease Triage","text":"Let's introduce some formal notation. Consider a dataset of historical readouts over m features X = x_1 ldots x_m, and let y denote the target variable that we want to predict. Assume that each experiment e in E yields readouts over a subset X_e subseteq X of features.","category":"page"},{"location":"tutorials/StaticDesigns.html","page":"Heart Disease Triage","title":"Heart Disease Triage","text":"Then, for each subset S subseteq E of experiments, we may model the value of information acquired by conducting the experiments in S as the accuracy of a predictive model that predicts the value of y based on readouts over features in X_S = bigcup_ein S X_e. Then this accuracy is our information value v_S of S.","category":"page"},{"location":"tutorials/StaticDesigns.html#Heart-Disease-Dataset","page":"Heart Disease Triage","title":"Heart Disease Dataset","text":"","category":"section"},{"location":"tutorials/StaticDesigns.html","page":"Heart Disease Triage","title":"Heart Disease Triage","text":"In this tutorial, we consider a dataset that includes 11 clinical features along with a binary variable indicating the presence of heart disease in patients. The dataset can be found at Kaggle: Heart Failure Prediction. It utilizes heart disease datasets from the UCI Machine Learning Repository.","category":"page"},{"location":"tutorials/StaticDesigns.html","page":"Heart Disease Triage","title":"Heart Disease Triage","text":"using CSV, DataFrames\ndata = CSV.File(\"data/heart_disease.csv\") |> DataFrame\ndata[1:10, :]","category":"page"},{"location":"tutorials/StaticDesigns.html#Predictive-Accuracy","page":"Heart Disease Triage","title":"Predictive Accuracy","text":"","category":"section"},{"location":"tutorials/StaticDesigns.html","page":"Heart Disease Triage","title":"Heart Disease Triage","text":"The CEEDesigns.jl package offers an additional flexibility by allowing an experiment to yield readouts over multiple features at the same time. In our scenario, we can consider the features RestingECG, Oldpeak, ST_Slope, and MaxHR to be obtained from a single experiment ECG.","category":"page"},{"location":"tutorials/StaticDesigns.html","page":"Heart Disease Triage","title":"Heart Disease Triage","text":"We specify the experiments along with the associated features:","category":"page"},{"location":"tutorials/StaticDesigns.html","page":"Heart Disease Triage","title":"Heart Disease Triage","text":"experiments = Dict(\n    # experiment => features\n    \"BloodPressure\" => [\"RestingBP\"],\n    \"ECG\" => [\"RestingECG\", \"Oldpeak\", \"ST_Slope\", \"MaxHR\"],\n    \"BloodCholesterol\" => [\"Cholesterol\"],\n    \"BloodSugar\" => [\"FastingBS\"],\n)","category":"page"},{"location":"tutorials/StaticDesigns.html","page":"Heart Disease Triage","title":"Heart Disease Triage","text":"We may also provide additional zero-cost features, which are always available.","category":"page"},{"location":"tutorials/StaticDesigns.html","page":"Heart Disease Triage","title":"Heart Disease Triage","text":"zero_cost_features = [\"Age\", \"Sex\", \"ChestPainType\", \"ExerciseAngina\"]","category":"page"},{"location":"tutorials/StaticDesigns.html","page":"Heart Disease Triage","title":"Heart Disease Triage","text":"And we specify the target for prediction.","category":"page"},{"location":"tutorials/StaticDesigns.html","page":"Heart Disease Triage","title":"Heart Disease Triage","text":"target = \"HeartDisease\"","category":"page"},{"location":"tutorials/StaticDesigns.html#Classifier","page":"Heart Disease Triage","title":"Classifier","text":"","category":"section"},{"location":"tutorials/StaticDesigns.html","page":"Heart Disease Triage","title":"Heart Disease Triage","text":"We use MLJ.jl to evaluate the predictive accuracy over subsets of experimental features.","category":"page"},{"location":"tutorials/StaticDesigns.html","page":"Heart Disease Triage","title":"Heart Disease Triage","text":"using MLJ\nimport BetaML, MLJModels\nusing Random: seed!","category":"page"},{"location":"tutorials/StaticDesigns.html","page":"Heart Disease Triage","title":"Heart Disease Triage","text":"We provide appropriate scientific types of the features.","category":"page"},{"location":"tutorials/StaticDesigns.html","page":"Heart Disease Triage","title":"Heart Disease Triage","text":"types = Dict(\n    :ChestPainType => Multiclass,\n    :Oldpeak => Continuous,\n    :HeartDisease => Multiclass,\n    :Age => Continuous,\n    :ST_Slope => Multiclass,\n    :RestingECG => Multiclass,\n    :RestingBP => Continuous,\n    :Sex => Multiclass,\n    :FastingBS => Continuous,\n    :ExerciseAngina => Multiclass,\n)\ndata = coerce(data, types);\nnothing #hide","category":"page"},{"location":"tutorials/StaticDesigns.html","page":"Heart Disease Triage","title":"Heart Disease Triage","text":"Next, we choose a particular predictive model that will evaluated in the sequel. We can list all models that are compatible with our dataset:","category":"page"},{"location":"tutorials/StaticDesigns.html","page":"Heart Disease Triage","title":"Heart Disease Triage","text":"models(matching(data, data[:, \"HeartDisease\"]))","category":"page"},{"location":"tutorials/StaticDesigns.html","page":"Heart Disease Triage","title":"Heart Disease Triage","text":"Eventually, we fix RandomForestClassifier from BetaML","category":"page"},{"location":"tutorials/StaticDesigns.html","page":"Heart Disease Triage","title":"Heart Disease Triage","text":"classifier = @load RandomForestClassifier pkg = BetaML verbosity = 3\nmodel = classifier(; n_trees = 20, max_depth = 10)","category":"page"},{"location":"tutorials/StaticDesigns.html#Performance-Evaluation","page":"Heart Disease Triage","title":"Performance Evaluation","text":"","category":"section"},{"location":"tutorials/StaticDesigns.html","page":"Heart Disease Triage","title":"Heart Disease Triage","text":"We use evaluate_experiments from CEEDesigns.StaticDesigns to evaluate the predictive accuracy over subsets of experiments. We use LogLoss as a measure of accuracy. It is possible to provide additional keyword arguments, which will be passed to MLJ.evaluate (such as measure, shown below).","category":"page"},{"location":"tutorials/StaticDesigns.html","page":"Heart Disease Triage","title":"Heart Disease Triage","text":"using CEEDesigns, CEEDesigns.StaticDesigns","category":"page"},{"location":"tutorials/StaticDesigns.html","page":"Heart Disease Triage","title":"Heart Disease Triage","text":"seed!(1) # evaluation process generally is not deterministic\nperf_eval = evaluate_experiments(\n    experiments,\n    model,\n    data[!, Not(\"HeartDisease\")],\n    data[!, \"HeartDisease\"];\n    zero_cost_features,\n    measure = LogLoss(),\n)","category":"page"},{"location":"tutorials/StaticDesigns.html","page":"Heart Disease Triage","title":"Heart Disease Triage","text":"We plot performance measures evaluated for subsets of experiments, sorted by performance measure.","category":"page"},{"location":"tutorials/StaticDesigns.html","page":"Heart Disease Triage","title":"Heart Disease Triage","text":"plot_evals(\n    perf_eval;\n    f = x -> sort(collect(keys(x)); by = k -> x[k], rev = true),\n    ylabel = \"logloss\",\n)","category":"page"},{"location":"tutorials/StaticDesigns.html#Cost-Efficient-Designs","page":"Heart Disease Triage","title":"Cost-Efficient Designs","text":"","category":"section"},{"location":"tutorials/StaticDesigns.html","page":"Heart Disease Triage","title":"Heart Disease Triage","text":"We specify the cost associated with the execution of each experiment.","category":"page"},{"location":"tutorials/StaticDesigns.html","page":"Heart Disease Triage","title":"Heart Disease Triage","text":"costs = Dict(\n    # experiment => cost\n    \"BloodPressure\" => 1,\n    \"ECG\" => 5,\n    \"BloodCholesterol\" => 20,\n    \"BloodSugar\" => 20,\n)","category":"page"},{"location":"tutorials/StaticDesigns.html","page":"Heart Disease Triage","title":"Heart Disease Triage","text":"We use the provided function efficient_designs to construct the set of cost-efficient experimental designs.","category":"page"},{"location":"tutorials/StaticDesigns.html","page":"Heart Disease Triage","title":"Heart Disease Triage","text":"designs = efficient_designs(costs, perf_eval)","category":"page"},{"location":"tutorials/StaticDesigns.html","page":"Heart Disease Triage","title":"Heart Disease Triage","text":"plot_front(designs; labels = make_labels(designs), ylabel = \"logloss\")","category":"page"},{"location":"tutorials/StaticDesigns.html#Parallel-Experiments","page":"Heart Disease Triage","title":"Parallel Experiments","text":"","category":"section"},{"location":"tutorials/StaticDesigns.html","page":"Heart Disease Triage","title":"Heart Disease Triage","text":"The previous example assumed that experiments had to be run sequentially. We can see how the optimal arrangement changes if we assume multiple experiments can be run in parallel. To that end, we first specify the monetary cost and execution time for each experiment, respectively.","category":"page"},{"location":"tutorials/StaticDesigns.html","page":"Heart Disease Triage","title":"Heart Disease Triage","text":"experiments_costs = Dict(\n    # experiment => (monetary cost, execution time) => features\n    \"BloodPressure\" => (1.0, 1.0) => [\"RestingBP\"],\n    \"ECG\" => (5.0, 5.0) => [\"RestingECG\", \"Oldpeak\", \"ST_Slope\", \"MaxHR\"],\n    \"BloodCholesterol\" => (20.0, 20.0) => [\"Cholesterol\"],\n    \"BloodSugar\" => (20.0, 20.0) => [\"FastingBS\"],\n)","category":"page"},{"location":"tutorials/StaticDesigns.html","page":"Heart Disease Triage","title":"Heart Disease Triage","text":"We set the maximum number of concurrent experiments. Additionally, we can specify the tradeoff between monetary cost and execution time. In our case, we aim to minimize the execution time.","category":"page"},{"location":"tutorials/StaticDesigns.html","page":"Heart Disease Triage","title":"Heart Disease Triage","text":"Below, we demonstrate the flexibility of efficient_designs as it can both evaluate the performance of experiments and generate efficient designs. Internally, evaluate_experiments is called first, followed by efficient_designs. Keyword arguments to the respective functions has to wrapped in eval_options and arrangement_options named tuples, respectively.","category":"page"},{"location":"tutorials/StaticDesigns.html","page":"Heart Disease Triage","title":"Heart Disease Triage","text":"# Implicit, calculates accuracies automatically.\nseed!(1) # evaluation process generally is not deterministic\ndesigns = efficient_designs(\n    experiments_costs,\n    model,\n    data[!, Not(\"HeartDisease\")],\n    data[!, \"HeartDisease\"];\n    eval_options = (; zero_cost_features, measure = LogLoss()),\n    arrangement_options = (; max_parallel = 2, tradeoff = (0.0, 1)),\n)","category":"page"},{"location":"tutorials/StaticDesigns.html","page":"Heart Disease Triage","title":"Heart Disease Triage","text":"As we can see, the algorithm correctly suggests running experiments in parallel.","category":"page"},{"location":"tutorials/StaticDesigns.html","page":"Heart Disease Triage","title":"Heart Disease Triage","text":"plot_front(designs; labels = make_labels(designs), ylabel = \"logloss\")","category":"page"}]
}
